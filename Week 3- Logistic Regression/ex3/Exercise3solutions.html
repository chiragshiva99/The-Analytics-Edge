<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Exercise 3 solutions</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

/* A workaround for https://github.com/jgm/pandoc/issues/4278 */
a.sourceLine {
  pointer-events: auto;
}

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">@font-face{font-family:"Open Sans";font-style:normal;font-weight:400;src:local("Open Sans"),local("OpenSans"),url(data:application/font-woff;base64,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) format("woff")}@font-face{font-family:"Open Sans";font-style:normal;font-weight:700;src:local("Open Sans Bold"),local("OpenSans-Bold"),url(data:application/font-woff;base64,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) format("woff")}*{box-sizing:border-box}body{padding:0;margin:0;font-family:"Open Sans","Helvetica Neue",Helvetica,Arial,sans-serif;font-size:16px;line-height:1.5;color:#606c71}a{color:#1e6bb8;text-decoration:none}a:hover{text-decoration:underline}.page-header{color:#fff;text-align:center;background-color:#159957;background-image:linear-gradient(120deg,#155799,#159957);padding:1.5rem 2rem}.page-header :last-child{margin-bottom:.5rem}@media screen and (max-width:42em){.page-header{padding:1rem 1rem}}.project-name{margin-top:0;margin-bottom:.1rem;font-size:2rem}@media screen and (max-width:42em){.project-name{font-size:1.75rem}}.project-tagline{margin-bottom:2rem;font-weight:400;opacity:.7;font-size:1.5rem}@media screen and (max-width:42em){.project-tagline{font-size:1.2rem}}.project-author,.project-date{font-weight:400;opacity:.7;font-size:1.2rem}@media screen and (max-width:42em){.project-author,.project-date{font-size:1rem}}.main-content,.toc{max-width:64rem;padding:2rem 4rem;margin:0 auto;font-size:1.1rem}.toc{padding-bottom:0}.toc .toc-box{padding:1.5rem;background-color:#f3f6fa;border:solid 1px #dce6f0;border-radius:.3rem}.toc .toc-box .toc-title{margin:0 0 .5rem;text-align:center}.toc .toc-box>ul{margin:0;padding-left:1.5rem}@media screen and (min-width:42em) and (max-width:64em){.toc{padding:2rem 2rem 0}}@media screen and (max-width:42em){.toc{padding:2rem 1rem 0;font-size:1rem}}.main-content :first-child{margin-top:0}@media screen and (min-width:42em) and (max-width:64em){.main-content{padding:2rem}}@media screen and (max-width:42em){.main-content{padding:2rem 1rem;font-size:1rem}}.main-content img{max-width:100%}.main-content h1,.main-content h2,.main-content h3,.main-content h4,.main-content h5,.main-content h6{margin-top:2rem;margin-bottom:1rem;font-weight:400;color:#159957}.main-content p{margin-bottom:1em}.main-content code{padding:2px 4px;font-family:Consolas,"Liberation Mono",Menlo,Courier,monospace;color:#567482;background-color:#f3f6fa;border-radius:.3rem}.main-content pre{padding:.8rem;margin-top:0;margin-bottom:1rem;font:1rem Consolas,"Liberation Mono",Menlo,Courier,monospace;color:#567482;word-wrap:normal;background-color:#f3f6fa;border:solid 1px #dce6f0;border-radius:.3rem;line-height:1.45;overflow:auto}@media screen and (max-width:42em){.main-content pre{font-size:.9rem}}.main-content pre>code{padding:0;margin:0;color:#567482;word-break:normal;white-space:pre;background:0 0;border:0}@media screen and (max-width:42em){.main-content pre>code{font-size:.9rem}}.main-content pre code,.main-content pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.main-content pre code:after,.main-content pre code:before,.main-content pre tt:after,.main-content pre tt:before{content:normal}.main-content ol,.main-content ul{margin-top:0}.main-content blockquote{padding:0 1rem;margin-left:0;color:#819198;border-left:.3rem solid #dce6f0;font-size:1.2rem}.main-content blockquote>:first-child{margin-top:0}.main-content blockquote>:last-child{margin-bottom:0}@media screen and (max-width:42em){.main-content blockquote{font-size:1.1rem}}.main-content table{width:100%;overflow:auto;word-break:normal;word-break:keep-all;-webkit-overflow-scrolling:touch;border-collapse:collapse;border-spacing:0;margin:1rem 0}.main-content table th{font-weight:700;background-color:#159957;color:#fff}.main-content table td,.main-content table th{padding:.5rem 1rem;border-bottom:1px solid #e9ebec;text-align:left}.main-content table tr:nth-child(odd){background-color:#f2f2f2}.main-content dl{padding:0}.main-content dl dt{padding:0;margin-top:1rem;font-size:1rem;font-weight:700}.main-content dl dd{padding:0;margin-bottom:1rem}.main-content hr{height:2px;padding:0;margin:1rem 0;background-color:#eff0f1;border:0}code span.kw { color: #a71d5d; font-weight: normal; } 
code span.dt { color: #795da3; } 
code span.dv { color: #0086b3; } 
code span.bn { color: #0086b3; } 
code span.fl { color: #0086b3; } 
code span.ch { color: #4070a0; } 
code span.st { color: #183691; } 
code span.co { color: #969896; font-style: italic; } 
code span.ot { color: #007020; } 
</style>





</head>

<body>




<section class="page-header">
<h1 class="title toc-ignore project-name">Exercise 3 solutions</h1>
<h4 class="date project-date">Term 5, 2022</h4>
</section>


<div id="TOC" class="toc">
<div class="toc-box">
<ul>
<li><a href="#question-1" id="toc-question-1"><span class="toc-section-number">1</span> Question 1</a>
<ul>
<li><a href="#a" id="toc-a"><span class="toc-section-number">1.1</span>
(a)</a></li>
<li><a href="#b" id="toc-b"><span class="toc-section-number">1.2</span>
(b)</a></li>
<li><a href="#c" id="toc-c"><span class="toc-section-number">1.3</span>
(c)</a></li>
<li><a href="#d" id="toc-d"><span class="toc-section-number">1.4</span>
(d)</a></li>
<li><a href="#e" id="toc-e"><span class="toc-section-number">1.5</span>
(e)</a></li>
<li><a href="#f" id="toc-f"><span class="toc-section-number">1.6</span>
(f)</a></li>
<li><a href="#g" id="toc-g"><span class="toc-section-number">1.7</span>
(g)</a></li>
<li><a href="#h" id="toc-h"><span class="toc-section-number">1.8</span>
(h)</a></li>
</ul></li>
<li><a href="#question-2" id="toc-question-2"><span class="toc-section-number">2</span> Question 2</a>
<ul>
<li><a href="#a-1" id="toc-a-1"><span class="toc-section-number">2.1</span> (a)</a></li>
<li><a href="#b-1" id="toc-b-1"><span class="toc-section-number">2.2</span> (b)</a></li>
<li><a href="#c-1" id="toc-c-1"><span class="toc-section-number">2.3</span> (c)</a></li>
<li><a href="#d-1" id="toc-d-1"><span class="toc-section-number">2.4</span> (d)</a></li>
<li><a href="#e-1" id="toc-e-1"><span class="toc-section-number">2.5</span> (e)</a></li>
<li><a href="#f-1" id="toc-f-1"><span class="toc-section-number">2.6</span> (f)</a></li>
<li><a href="#g-1" id="toc-g-1"><span class="toc-section-number">2.7</span> (g)</a></li>
<li><a href="#h-1" id="toc-h-1"><span class="toc-section-number">2.8</span> (h)</a></li>
<li><a href="#i" id="toc-i"><span class="toc-section-number">2.9</span>
(i)</a></li>
<li><a href="#j" id="toc-j"><span class="toc-section-number">2.10</span>
(j)</a></li>
<li><a href="#k" id="toc-k"><span class="toc-section-number">2.11</span>
(k)</a></li>
<li><a href="#l" id="toc-l"><span class="toc-section-number">2.12</span>
(l)</a></li>
<li><a href="#m" id="toc-m"><span class="toc-section-number">2.13</span>
(m)</a></li>
<li><a href="#n" id="toc-n"><span class="toc-section-number">2.14</span>
(n)</a></li>
<li><a href="#o" id="toc-o"><span class="toc-section-number">2.15</span>
(o)</a></li>
</ul></li>
<li><a href="#question-3" id="toc-question-3"><span class="toc-section-number">3</span> Question 3</a>
<ul>
<li><a href="#a-2" id="toc-a-2"><span class="toc-section-number">3.1</span> (a)</a></li>
<li><a href="#b-2" id="toc-b-2"><span class="toc-section-number">3.2</span> (b)</a></li>
<li><a href="#c-2" id="toc-c-2"><span class="toc-section-number">3.3</span> (c)</a></li>
<li><a href="#d-2" id="toc-d-2"><span class="toc-section-number">3.4</span> (d)</a></li>
<li><a href="#e-2" id="toc-e-2"><span class="toc-section-number">3.5</span> (e)</a></li>
<li><a href="#f-2" id="toc-f-2"><span class="toc-section-number">3.6</span> (f)</a></li>
<li><a href="#g-2" id="toc-g-2"><span class="toc-section-number">3.7</span> (g)</a></li>
<li><a href="#h-2" id="toc-h-2"><span class="toc-section-number">3.8</span> (h)</a></li>
<li><a href="#i-1" id="toc-i-1"><span class="toc-section-number">3.9</span> (i)</a></li>
<li><a href="#j-1" id="toc-j-1"><span class="toc-section-number">3.10</span> (j)</a></li>
<li><a href="#k-1" id="toc-k-1"><span class="toc-section-number">3.11</span> (k)</a></li>
<li><a href="#l-1" id="toc-l-1"><span class="toc-section-number">3.12</span> (l)</a></li>
<li><a href="#m-1" id="toc-m-1"><span class="toc-section-number">3.13</span> (m)</a></li>
<li><a href="#n-1" id="toc-n-1"><span class="toc-section-number">3.14</span> (n)</a></li>
<li><a href="#o-1" id="toc-o-1"><span class="toc-section-number">3.15</span> (o)</a></li>
<li><a href="#p" id="toc-p"><span class="toc-section-number">3.16</span>
(p)</a></li>
<li><a href="#q" id="toc-q"><span class="toc-section-number">3.17</span>
(q)</a></li>
</ul></li>
<li><a href="#question-4" id="toc-question-4"><span class="toc-section-number">4</span> Question 4</a>
<ul>
<li><a href="#a-3" id="toc-a-3"><span class="toc-section-number">4.1</span> a)</a></li>
<li><a href="#b-3" id="toc-b-3"><span class="toc-section-number">4.2</span> b)</a></li>
<li><a href="#c-3" id="toc-c-3"><span class="toc-section-number">4.3</span> c)</a></li>
<li><a href="#d-3" id="toc-d-3"><span class="toc-section-number">4.4</span> d)</a></li>
<li><a href="#e-3" id="toc-e-3"><span class="toc-section-number">4.5</span> e)</a></li>
<li><a href="#f-3" id="toc-f-3"><span class="toc-section-number">4.6</span> f)</a></li>
<li><a href="#g-3" id="toc-g-3"><span class="toc-section-number">4.7</span> g)</a></li>
<li><a href="#h-3" id="toc-h-3"><span class="toc-section-number">4.8</span> h)</a></li>
<li><a href="#i-2" id="toc-i-2"><span class="toc-section-number">4.9</span> i)</a></li>
<li><a href="#j-2" id="toc-j-2"><span class="toc-section-number">4.10</span> j)</a></li>
<li><a href="#k-2" id="toc-k-2"><span class="toc-section-number">4.11</span> k)</a></li>
<li><a href="#l-2" id="toc-l-2"><span class="toc-section-number">4.12</span> l)</a></li>
<li><a href="#m-2" id="toc-m-2"><span class="toc-section-number">4.13</span> m)</a></li>
<li><a href="#n-2" id="toc-n-2"><span class="toc-section-number">4.14</span> n)</a></li>
<li><a href="#o-2" id="toc-o-2"><span class="toc-section-number">4.15</span> o)</a></li>
</ul></li>
</ul>
</div>
</div>

<section class="main-content">
<div id="question-1" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Question 1</h1>
<div id="a" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> (a)</h2>
<p>Q: Each row in the baseball dataset represents a team in a particular
year. Read the data into a dataframe called
<code>baseballlarge</code>.</p>
<p>A:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>baseballlarge <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;baseballlarge.csv&quot;</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># str(baseballlarge)</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># while we will be commenting out all str(data.frame) calls</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># it is still advisable to take at least one look at it</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># but it is often not directly relevant to questions</span></span></code></pre></div>
<div id="i." class="section level3" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> i.</h3>
<p>Q: How many team/year pairs are there in the whole dataset?</p>
<p>A:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># number of observations equal to number of rows</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(baseballlarge)</span></code></pre></div>
<pre><code>## [1] 1232</code></pre>
<p>There are a total of 1232 team/year pairs in the whole dataset.</p>
</div>
<div id="ii." class="section level3" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> ii.</h3>
<p>Q: Though the dataset contains data from 1962 until 2012, we removed
several years with shorter-than-usual seasons. Using the
<code>table()</code> function, identify the total number of years
included in this dataset.</p>
<p>A:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># number of entries can be counted with length()</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(<span class="fu">table</span>(baseballlarge<span class="sc">$</span>Year)) </span></code></pre></div>
<pre><code>## [1] 47</code></pre>
<p>There are a total of 47 years included in the dataset though it
ranges from 1962 to 2012.</p>
</div>
<div id="iii." class="section level3" number="1.1.3">
<h3><span class="header-section-number">1.1.3</span> iii.</h3>
<p>Q: Since we are only analyzing teams that made the playoffs, use the
<code>subset()</code> function to create a smaller data frame limited to
teams that made the playoffs. Your subsetted data frame should still be
called <code>baseballlarge</code>. How many team/year pairs are included
in the new dataset?</p>
<p>A:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>baseballlarge <span class="ot">&lt;-</span> <span class="fu">subset</span>(baseballlarge, Playoffs <span class="sc">==</span> <span class="dv">1</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(baseballlarge)</span></code></pre></div>
<pre><code>## [1] 244</code></pre>
<p>There are a total of 244 team/year pairs in the new dataset.</p>
</div>
<div id="iv." class="section level3" number="1.1.4">
<h3><span class="header-section-number">1.1.4</span> iv.</h3>
<p>Q: Through the years, different numbers of teams have been invited to
the playoffs. Find the different number of teams making the playoffs
across the seasons.</p>
<p>A: The following code shows the number of teams at the playoffs over
the years.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(baseballlarge<span class="sc">$</span>Year)</span></code></pre></div>
<pre><code>## 
## 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1973 1974 1975 1976 1977 1978 
##    2    2    2    2    2    2    2    4    4    4    4    4    4    4    4    4 
## 1979 1980 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1996 1997 
##    4    4    4    4    4    4    4    4    4    4    4    4    4    4    8    8 
## 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 
##    8    8    8    8    8    8    8    8    8    8    8    8    8    8   10</code></pre>
<p>The top row is the year, and the bottom row is the number of
teams.</p>
</div>
</div>
<div id="b" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> (b)</h2>
<p>It is much harder to win the World Series if there are 10 teams
competing for the championship versus just two. Therefore, we will add
the predictor variable <code>NumCompetitors</code> to the data frame.
<code>NumCompetitors</code> will contain the number of total teams
making the playoffs in the year of a particular team/year pair. For
instance, <code>NumCompetitors</code> should be 2 for the 1962 New York
Yankees, but it should be 8 for the 1998 Boston Red Sox. We want to look
up the number of teams in the playoffs for each team/year pair in the
dataset, and store it as a new variable named
<code>NumCompetitors</code> in the data frame. Do this. How many playoff
team/year pairs are there in the dataset from years where 8 teams were
invited to the playoffs?</p>
<p>A:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>year_col <span class="ot">&lt;-</span> baseballlarge<span class="sc">$</span>Year</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">#the column of Year values, given by as.character(year_col), are taken and mapped to values as found in table(year_col). They are then assigned to a new column NumCompetitors</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>baseballlarge<span class="sc">$</span>NumCompetitors <span class="ot">&lt;-</span> <span class="fu">table</span>(year_col)[<span class="fu">as.character</span>(year_col)]</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(baseballlarge<span class="sc">$</span>NumCompetitors)</span></code></pre></div>
<pre><code>## 
##   2   4   8  10 
##  14  92 128  10</code></pre>
<p>To retrieve the number directly:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(baseballlarge<span class="sc">$</span>NumCompetitors)[<span class="st">&quot;8&quot;</span>]  <span class="co">#this lets us see the value 8, and count 128</span></span></code></pre></div>
<pre><code>##   8 
## 128</code></pre>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">unname</span>(<span class="fu">table</span>(baseballlarge<span class="sc">$</span>NumCompetitors)[<span class="st">&quot;8&quot;</span>]) <span class="co"># with unname(), we can retrieve 128 directly</span></span></code></pre></div>
<pre><code>## [1] 128</code></pre>
<p>There were 128 team/year pairs where 8 teams were invited to the
playoffs. (Note that we can also verify this with <a href="#oneaiv">iv.</a> as 8 * 16 = 128)</p>
</div>
<div id="c" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> (c)</h2>
<p>Q: In this problem, we seek to predict whether a team won the World
Series; in our dataset this is denoted with a <code>RankPlayoffs</code>
value of 1. Add a variable named <code>WorldSeries</code> to the data
frame that takes value 1 if a team won the World Series in the indicated
year and a 0 otherwise. How many observations do we have in our dataset
where a team did NOT win the World Series?</p>
<p>A:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>baseballlarge<span class="sc">$</span>WorldSeries <span class="ot">&lt;-</span> <span class="fu">as.integer</span>(baseballlarge<span class="sc">$</span>RankPlayoffs <span class="sc">==</span> <span class="dv">1</span>) <span class="co">#as.integer converts TRUE to 1 and FALSE to 0</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(baseballlarge<span class="sc">$</span>WorldSeries)</span></code></pre></div>
<pre><code>## 
##   0   1 
## 197  47</code></pre>
<p>Let’s retrieve the value without “0” as the header,</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">unname</span>(<span class="fu">table</span>(baseballlarge<span class="sc">$</span>WorldSeries)[<span class="st">&quot;0&quot;</span>])</span></code></pre></div>
<pre><code>## [1] 197</code></pre>
<p>There are 197 team/year pairs in the dataset who did not win the
World Series.</p>
</div>
<div id="d" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> (d)</h2>
<p>Q: When we are not sure which of our variables are useful in
predicting a particular outcome, it is often helpful to build simple
models, which are models that predict the outcome using a single
independent variable. Which of the variables is a significant predictor
of the <code>WorldSeries</code> variable in a logistic regression model?
To determine significance, remember to look at the stars in the summary
output of the model. We’ll define an independent variable as significant
if there is at least one star at the end of the coefficients row for
that variable (this is equivalent to the p-value column having a value
smaller than 0.05). Note that you have to build multiple models - with
each model having a single independent variable from (this is equivalent
to the probability column having a value smaller than 0.05). Note that
you have to build multiple models ( <code>Year</code>, <code>RS</code>,
<code>RA</code>, <code>W</code>, <code>OBP</code>, <code>SLG</code>,
<code>BA</code>, <code>RankSeason</code>, <code>NumCompetitors</code>,
<code>League</code>) to answer this question (you can code the
<code>League</code> variable as a categorical variable). Use the
dataframe <code>baseballlarge</code> to build the models.</p>
<p>A: First of all we will do this in a simple manner before trying an
approach that scales better:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>model_1_d <span class="ot">&lt;-</span> <span class="fu">glm</span>(WorldSeries <span class="sc">~</span> Year, <span class="at">data =</span> baseballlarge, <span class="at">family =</span> binomial)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model_1_d)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = WorldSeries ~ Year, family = binomial, data = baseballlarge)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.0297  -0.6797  -0.5435  -0.4648   2.1504  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept) 72.23602   22.64409    3.19  0.00142 **
## Year        -0.03700    0.01138   -3.25  0.00115 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 239.12  on 243  degrees of freedom
## Residual deviance: 228.35  on 242  degrees of freedom
## AIC: 232.35
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>As can be seen, a massive output is printed. While IQR, deviance and
so on are useful in general, they are not critical to the task as given
by the question. Our aim is simply to check the p-value against
0.05.</p>
<p>As a tip, check what named elements a named list has in RStudio by
using <code>$</code>:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># in this example we will just grab a random element from the summary</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">summary</span>(model_1_d)  <span class="co"># to make a short-named new variable</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>x<span class="sc">$</span>iter <span class="co"># simply type &quot;x$&quot; and wait for RStudio to suggest things</span></span></code></pre></div>
<pre><code>## [1] 4</code></pre>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># number of Newton-Raphson iterations, good to know but not critical</span></span></code></pre></div>
<p>We can take the p-value directly from the <code>summary(model)</code>
object directly:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we use [2, 4] as following models will only have 1 variable anyway</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="co"># one can get the p-value column using [,4]</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co"># &#39;coefficients&#39; can be shortened to &#39;coef&#39; i.e. summary(model1)$coef</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co"># assuming no other elements named &#39;coef&#39; or similar were added</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model_1_d)<span class="sc">$</span>coefficients[<span class="dv">2</span>, <span class="dv">4</span>] </span></code></pre></div>
<pre><code>## [1] 0.001154469</code></pre>
<p>Scaling up, we write a for-loop to do this for all variables.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># initialise some containers we will add to later</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>p_val_1_d <span class="ot">&lt;-</span> <span class="fu">c</span>() <span class="co">#to save our p-values for later</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>model_list_1 <span class="ot">&lt;-</span> <span class="fu">list</span>()  <span class="co"># to contain all the models later</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>all_vars_1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Year&quot;</span>, <span class="st">&quot;RS&quot;</span>, <span class="st">&quot;RA&quot;</span>, <span class="st">&quot;W&quot;</span>, <span class="st">&quot;OBP&quot;</span>, <span class="st">&quot;SLG&quot;</span>, <span class="st">&quot;BA&quot;</span>,</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>                <span class="st">&quot;RankSeason&quot;</span>, <span class="st">&quot;NumCompetitors&quot;</span>, <span class="st">&quot;League&quot;</span>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co">#to train single-variable models                                                                                           </span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (variable <span class="cf">in</span> all_vars_1) {</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    model <span class="ot">&lt;-</span> <span class="fu">glm</span>(<span class="fu">as.formula</span>(<span class="fu">paste0</span>(<span class="st">&quot;WorldSeries ~ &quot;</span>, variable)),</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>                 <span class="at">data =</span> baseballlarge, <span class="at">family =</span> binomial)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    model_list_1[[variable]] <span class="ot">&lt;-</span> model <span class="co">#save the trained model in the list</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we are appending a named numeric variable, for reference later</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    p_val_1_d <span class="ot">&lt;-</span> <span class="fu">c</span>(p_val_1_d,</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>                   <span class="fu">setNames</span>(<span class="fu">summary</span>(model)<span class="sc">$</span>coefficients[<span class="dv">2</span>, <span class="dv">4</span>], variable))  </span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>p_val_1_d</span></code></pre></div>
<pre><code>##           Year             RS             RA              W            OBP 
##   0.0011544695   0.2013364471   0.0262137758   0.0577260348   0.2959201177 
##            SLG             BA     RankSeason NumCompetitors         League 
##   0.0504256593   0.8385900817   0.0438313556   0.0006784323   0.6264465507</code></pre>
<p>We directly get the significance variables at the 5% significance
level with the following code:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>sig_vars_1_d <span class="ot">&lt;-</span> <span class="fu">names</span>(p_val_1_d[p_val_1_d <span class="sc">&lt;</span> <span class="fl">0.05</span>])</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>sig_vars_1_d</span></code></pre></div>
<pre><code>## [1] &quot;Year&quot;           &quot;RA&quot;             &quot;RankSeason&quot;     &quot;NumCompetitors&quot;</code></pre>
<p>The significant (at the 5% significance level) variables are
<code>Year</code>, <code>RA</code>, <code>RankSeason</code> and
<code>NumCompetitors</code>. However, by manual inspection of the
p-values, we can see that <code>W</code> and <code>SLG</code> are quite
close with p-values 0.0577 and 0.0504 respectively which are just a
little short of 0.05.</p>
</div>
<div id="e" class="section level2" number="1.5">
<h2><span class="header-section-number">1.5</span> (e)</h2>
<p>Q: In this question, we will consider multivariate models that
combine the variables we found to be significant in (d). Build a model
using all of the variables that you found to be significant in (d). How
many variables are significant in the combined model?</p>
<p>A: Note that while we will include the variables <code>Year</code>,
<code>RA</code>, <code>RankSeason</code> and
<code>NumCompetitors</code>, it also makes sense to include
<code>W</code> and <code>SLG</code> since they have p-values close to
0.05.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>sig_vars_1_e <span class="ot">&lt;-</span> <span class="fu">c</span>(sig_vars_1_d, <span class="st">&quot;W&quot;</span>, <span class="st">&quot;SLG&quot;</span>) <span class="co"># add on manually considered W and SLG</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="co"># with this we can make the formula without manually typing all the variable names</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="co"># we need 2 paste0() as the sig_vars_1_d  are a vector of characters</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="co"># we need to collapse sig_vars_1_e into string &quot;Year + ... + SLG&quot;</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>formula_1 <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(<span class="fu">paste0</span>(<span class="st">&quot;WorldSeries ~ &quot;</span>,</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>                               <span class="fu">paste0</span>(sig_vars_1_e, <span class="at">collapse =</span> <span class="st">&quot;+&quot;</span>)))</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>model_1_e <span class="ot">&lt;-</span> <span class="fu">glm</span>(formula_1, <span class="at">data =</span> baseballlarge, <span class="at">family =</span> binomial)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="co"># p-values</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>p_val_1_e <span class="ot">&lt;-</span> <span class="fu">summary</span>(model_1_e)<span class="sc">$</span>coefficients[,<span class="dv">4</span>]</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>p_val_1_e</span></code></pre></div>
<pre><code>##    (Intercept)           Year             RA     RankSeason NumCompetitors 
##      0.9058395      0.9884236      0.8760078      0.6144398      0.2086618 
##              W            SLG 
##      0.2671839      0.8390221</code></pre>
<p>We can check for significance at the 5% significance level quite
simply:</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(p_val_1_e[p_val_1_e <span class="sc">&lt;</span> <span class="fl">0.05</span>])</span></code></pre></div>
<pre><code>## character(0)</code></pre>
<p>Unfortunately, it seems that in this new multivariate model, none of
the variables are significant. You can check that this is the same
whether we added <code>W</code> and <code>SLG</code> or not.</p>
</div>
<div id="f" class="section level2" number="1.6">
<h2><span class="header-section-number">1.6</span> (f)</h2>
<p>Q: Often, variables that were significant in single variable models
are no longer significant in multivariate analysis due to correlation
between the variables. Are there any such variables in this example?
Which of the variable pairs have a high degree of correlation (a
correlation greater than 0.8 or less than -0.8)?</p>
<p>A: We can run the <code>cor()</code> function first:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>corr_1 <span class="ot">&lt;-</span> <span class="fu">cor</span>(baseballlarge[,sig_vars_1_d])</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>corr_1</span></code></pre></div>
<pre><code>##                     Year        RA RankSeason NumCompetitors
## Year           1.0000000 0.4762422  0.3852191      0.9139548
## RA             0.4762422 1.0000000  0.3991413      0.5136769
## RankSeason     0.3852191 0.3991413  1.0000000      0.4247393
## NumCompetitors 0.9139548 0.5136769  0.4247393      1.0000000</code></pre>
<p>The answer is already in the above, and we can just read through the
non-diagonal terms to check if they are &gt; 0.8. But here, we will show
an approach which gets the variable names directly.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">diag</span>(corr_1) <span class="ot">&lt;-</span> <span class="dv">0</span>  <span class="co"># self-correlation not relevant</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="fu">row.names</span>(<span class="fu">which</span>(corr_1 <span class="sc">&gt;</span> <span class="fl">0.8</span>, <span class="at">arr.ind =</span> T))</span></code></pre></div>
<pre><code>## [1] &quot;NumCompetitors&quot; &quot;Year&quot;</code></pre>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># x &lt;- row.names(which(corr_1 &gt; 0.8, arr.ind = T))  # NOT DISPLAYED</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="co"># x</span></span></code></pre></div>
<p>The variables which are highly correlated are
<code>NumCompetitors</code> and <code>Year</code>.</p>
</div>
<div id="g" class="section level2" number="1.7">
<h2><span class="header-section-number">1.7</span> (g)</h2>
<p>Q: Build all of the two variable models using variables in (e). You
should compare them with the single variable models from (d). Which
model has the best AIC value (the minimum AIC value)?</p>
<p>A: We repeat the procedure in <a href="#d-2">(d)</a>.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>model_1_g <span class="ot">&lt;-</span> <span class="fu">glm</span>(WorldSeries <span class="sc">~</span> Year <span class="sc">+</span> RA,</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>                 <span class="at">data =</span> baseballlarge, <span class="at">family =</span> binomial)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model_1_g)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = WorldSeries ~ Year + RA, family = binomial, data = baseballlarge)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.0402  -0.6878  -0.5298  -0.4785   2.1370  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) 63.610741  25.654830   2.479   0.0132 *
## Year        -0.032084   0.013323  -2.408   0.0160 *
## RA          -0.001766   0.002585  -0.683   0.4945  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 239.12  on 243  degrees of freedom
## Residual deviance: 227.88  on 241  degrees of freedom
## AIC: 233.88
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>A lot of output, but in this part we are interested in the Akaike
Information Criterion (AIC). After this, we will simply try to
<em>not</em> display the raw output of <code>summary()</code> again.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model_1_g)<span class="sc">$</span>aic  <span class="co"># we can use this to grab AIC directly</span></span></code></pre></div>
<pre><code>## [1] 233.8766</code></pre>
<p>Instead of typing all the pairwise combinations of variables, we show
an iterative approach below.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>var_combns_1 <span class="ot">&lt;-</span> <span class="fu">combn</span>(sig_vars_1_d, <span class="dv">2</span>)  <span class="co"># combinations of 2 variables</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>aic_table_1 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">var_1 =</span> <span class="fu">character</span>(), <span class="at">var_2 =</span> <span class="fu">character</span>(),</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>                          <span class="at">aic =</span> <span class="fu">numeric</span>())</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="co">#iterate through the pariwise combinations of variables</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (idx <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">choose</span>(<span class="fu">length</span>(sig_vars_1_d), <span class="dv">2</span>)) {</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>    var_comb <span class="ot">&lt;-</span> var_combns_1[,idx] <span class="co">#get the combination of variables</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>    model <span class="ot">&lt;-</span> <span class="fu">glm</span>(<span class="fu">formula</span>(<span class="fu">paste0</span>(<span class="st">&quot;WorldSeries ~ &quot;</span>, <span class="fu">paste0</span>(var_comb, <span class="at">collapse =</span> <span class="st">&quot;+&quot;</span>))),</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>             <span class="at">data =</span> baseballlarge, <span class="at">family =</span> binomial)</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># note that rbind is relatively slow but this is not too important</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>    aic_table_1 <span class="ot">&lt;-</span> <span class="fu">rbind</span>(aic_table_1,</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>                         <span class="fu">data.frame</span>(<span class="at">var1 =</span> var_comb[<span class="dv">1</span>],</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">var2 =</span> var_comb[<span class="dv">2</span>],</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">aic =</span> <span class="fu">summary</span>(model)<span class="sc">$</span>aic))</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a><span class="co">#iterate through all the original variables (single variables)</span></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (model <span class="cf">in</span> model_list_1) {</span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a>    aic_table_1 <span class="ot">&lt;-</span> <span class="fu">rbind</span>(aic_table_1,</span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>                         <span class="fu">data.frame</span>(<span class="at">var1 =</span> <span class="fu">names</span>(model<span class="sc">$</span>coefficients)[<span class="dv">2</span>],</span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">var2 =</span> <span class="cn">NA</span>,  <span class="co"># NULL does not work</span></span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">aic =</span> model<span class="sc">$</span>aic))</span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>aic_table_1</span></code></pre></div>
<pre><code>##              var1           var2      aic
## 1            Year             RA 233.8766
## 2            Year     RankSeason 233.5524
## 3            Year NumCompetitors 232.8972
## 4              RA     RankSeason 238.2180
## 5              RA NumCompetitors 232.7429
## 6      RankSeason NumCompetitors 232.5240
## 7            Year           &lt;NA&gt; 232.3508
## 8              RS           &lt;NA&gt; 241.4543
## 9              RA           &lt;NA&gt; 237.8839
## 10              W           &lt;NA&gt; 239.5105
## 11            OBP           &lt;NA&gt; 242.0209
## 12            SLG           &lt;NA&gt; 239.2283
## 13             BA           &lt;NA&gt; 243.0804
## 14     RankSeason           &lt;NA&gt; 238.7546
## 15 NumCompetitors           &lt;NA&gt; 230.9592
## 16       LeagueNL           &lt;NA&gt; 242.8846</code></pre>
<p>As usual, there are a lot of numbers to look through. To simplify
things we can just print the row with minimum AIC:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>aic_table_1[<span class="fu">which.min</span>(aic_table_1[,<span class="dv">3</span>]),]</span></code></pre></div>
<pre><code>##              var1 var2      aic
## 15 NumCompetitors &lt;NA&gt; 230.9592</code></pre>
<p>This shows that the model <code>World Series ~ NumCompetitors</code>
has the best AIC value.</p>
</div>
<div id="h" class="section level2" number="1.8">
<h2><span class="header-section-number">1.8</span> (h)</h2>
<p>Q: Comment on your results.</p>
<p>A: Disappointingly, and somewhat unsurprisingly, it seems that in the
winning of playoffs, the number of competitors is the single best and
strongest predictor. The other predictors such as Wins, Runs Scored,
Runs Allowed, On-Base Percentage, Slugging Percentage and Batting
Average do not seem to be as good (linear) predictors.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co">#use this to help you clear your environment :)</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="fu">setdiff</span>(<span class="fu">ls</span>(), <span class="fu">ls</span>(<span class="at">pattern =</span> <span class="st">&quot;SETUP&quot;</span>))</span></code></pre></div>
<pre><code>##  [1] &quot;aic_table_1&quot;   &quot;all_vars_1&quot;    &quot;baseballlarge&quot; &quot;corr_1&quot;       
##  [5] &quot;formula_1&quot;     &quot;idx&quot;           &quot;model&quot;         &quot;model_1_d&quot;    
##  [9] &quot;model_1_e&quot;     &quot;model_1_g&quot;     &quot;model_list_1&quot;  &quot;p_val_1_d&quot;    
## [13] &quot;p_val_1_e&quot;     &quot;sig_vars_1_d&quot;  &quot;sig_vars_1_e&quot;  &quot;var_comb&quot;     
## [17] &quot;var_combns_1&quot;  &quot;variable&quot;      &quot;x&quot;             &quot;year_col&quot;</code></pre>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">setdiff</span>(<span class="fu">ls</span>(), <span class="fu">ls</span>(<span class="at">pattern =</span> <span class="st">&quot;SETUP&quot;</span>)))</span></code></pre></div>
</div>
</div>
<div id="question-2" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Question 2</h1>
<div id="a-1" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> (a)</h2>
<p>Q: Load the dataset <code>Parole.csv</code> into a data frame called
<code>Parole</code>. How many parolees are contained in the dataset?</p>
<p>A:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>Parole <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Parole.csv&quot;</span>)</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="co"># str(Parole)</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(Parole)</span></code></pre></div>
<pre><code>## [1] 675</code></pre>
<p>There are a total of 675 parolees in this dataset.</p>
</div>
<div id="b-1" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> (b)</h2>
<p>Q: How many of the parolees in the dataset violated the terms of
their parole?</p>
<p>A:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">unname</span>(<span class="fu">table</span>(Parole<span class="sc">$</span>Violator)[<span class="dv">2</span>])  <span class="co"># col of &quot;1&quot; is the second element</span></span></code></pre></div>
<pre><code>## [1] 78</code></pre>
<p>78 of the parolees violated the terms of their parole.</p>
</div>
<div id="c-1" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> (c)</h2>
<p>Q: Factor variables are variables that take on a discrete set of
values and can be either unordered or ordered. Names of countries
indexed by levels is an example of an unordered factor because there
isn’t any natural ordering between the levels. An ordered factor has a
natural ordering between the levels (an example would be the
classifications “large”, “medium” and “small”). Which variables in this
dataset are unordered factors with at least three levels? To deal with
unordered factors in a regression model, the standard practice is to
define one level as the “reference level” and create a binary variable
for each of the remaining levels. In doing so, a factor with <em>n</em>
levels is replaced by <em>n</em>-1 binary variables. We will see this in
question <a href="#e-2">(e)</a>.</p>
<p>A:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We observe that there are 2 columns with factor type, State and Crime</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: read.csv automatically converts characters to strings unless &#39;stringsAsFactors=F&#39; is specified</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sapply</span>(Parole, class)</span></code></pre></div>
<pre><code>##             Male        RaceWhite              Age            State 
##        &quot;integer&quot;        &quot;integer&quot;        &quot;numeric&quot;      &quot;character&quot; 
##       TimeServed      MaxSentence MultipleOffenses            Crime 
##        &quot;numeric&quot;        &quot;integer&quot;        &quot;integer&quot;      &quot;character&quot; 
##         Violator 
##        &quot;integer&quot;</code></pre>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># If there is a column with character type you wish to convert eg.State, use: Parole$State &lt;- as.factor(Parole$State)</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sapply</span>(Parole[<span class="fu">sapply</span>(Parole, is.factor)], nlevels) <span class="co">#get the number of levels of each column</span></span></code></pre></div>
<pre><code>## named list()</code></pre>
<p>In this data, <code>State</code> and <code>Crime</code> are unordered
factor variables with at least 3 variables (4 each).</p>
</div>
<div id="d-1" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> (d)</h2>
<p>Q: To ensure consistent training/testing set splits, run the
following 5 lines of code (do not include the line numbers at the
beginning):</p>
<p><code>(1) &gt; set.seed(144)</code>
<code>(2) &gt; library(caTools)</code>
<code>(3) &gt; split &lt;- sample.split(Parole$Violator, SplitRatio = 0.7)</code>
<code>(4) &gt; train &lt;- subset(Parole, split == TRUE)</code>
<code>(5) &gt; test &lt;- subset(Parole, split == FALSE)</code></p>
<p>Roughly what proportion of parolees have been allocated to the
training and testing sets?</p>
<p>Now, suppose you re-ran lines (1)-(5) again. What would you
expect?</p>
<ul>
<li>The exact same training/testing set split as the first execution of
(1)-(5)</li>
<li>A different training/testing set split from the first execution of
(1)-(5)</li>
</ul>
<p>If you instead ONLY re-ran lines (3)-(5), what would you expect?</p>
<ul>
<li>The exact same training/testing set split as the first execution of
(1)-(5)</li>
<li>A different training/testing set split from the first execution of
(1)-(5)</li>
</ul>
<p>If you instead called <code>set.seed()</code> with a different number
and then re-ran lines (3)-(5), what would you expect?</p>
<ul>
<li>The exact same training/testing set split as the first execution of
(1)-(5)</li>
<li>A different training/testing set split from the first execution of
(1)-(5)</li>
</ul>
<p>A: In the split, roughly 70% of the parolees have been assigned to
the training and 30% to the test set. If you rerun the commands (1) -
(5), we would expect to get the same training/test split as the first
execution, and this is because we set the random seed (used by the
random number generator) to be the same. It follows naturally that if we
only run commands (3) - (5) without setting a seed, or setting a seed to
a different number from 144, we would get a different training/test
split.</p>
<p>All of this can be verified fairly easily, although we will use
different variable names:</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">144</span>)</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caTools)</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>s1 <span class="ot">&lt;-</span> <span class="fu">sample.split</span>(Parole<span class="sc">$</span>Violator, <span class="at">SplitRatio =</span> <span class="fl">0.7</span>)</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>tr1 <span class="ot">&lt;-</span> <span class="fu">subset</span>(Parole, s1 <span class="sc">==</span> <span class="cn">TRUE</span>)</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>te1 <span class="ot">&lt;-</span> <span class="fu">subset</span>(Parole, s1 <span class="sc">==</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>The second split,</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">144</span>)</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caTools)</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>s2 <span class="ot">&lt;-</span> <span class="fu">sample.split</span>(Parole<span class="sc">$</span>Violator, <span class="at">SplitRatio =</span> <span class="fl">0.7</span>)</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>tr2 <span class="ot">&lt;-</span> <span class="fu">subset</span>(Parole, s2 <span class="sc">==</span> <span class="cn">TRUE</span>)</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>te2 <span class="ot">&lt;-</span> <span class="fu">subset</span>(Parole, s2 <span class="sc">==</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>Now we compare them:</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="fu">identical</span>(s1, s2, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="fu">identical</span>(tr1, tr2, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="fu">identical</span>(te1, te2, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## [1] TRUE
## [1] TRUE
## [1] TRUE</code></pre>
<p>If anything but three <code>TRUE</code>s were returned, then
something would have been wrong. One may opt to check with some
memory-checking functions that the objects above being checked for being
the same are actually residing in different parts of the computer’s
memory.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>
(Else if they were the same object in memory, it would make sense they
would pass checks of being identical.) Further, running
<code>library(caTools)</code> is not required other than the first time,
and should not affect the splits.</p>
<p>Next we just check without setting the seed:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>s3 <span class="ot">&lt;-</span> <span class="fu">sample.split</span>(Parole<span class="sc">$</span>Violator, <span class="at">SplitRatio =</span> <span class="fl">0.7</span>)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>tr3 <span class="ot">&lt;-</span> <span class="fu">subset</span>(Parole, s3 <span class="sc">==</span> <span class="cn">TRUE</span>)</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>te3 <span class="ot">&lt;-</span> <span class="fu">subset</span>(Parole, s3 <span class="sc">==</span> <span class="cn">FALSE</span>)</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="fu">identical</span>(s1, s3, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## [1] FALSE</code></pre>
<p>We can check further with a different seed:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">3</span>) <span class="co"># also equal to  569936821221962380720**3 +</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>            <span class="co">#               -569936821113563493509**3 +</span></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>            <span class="co">#               -472715493453327032   **3</span></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a><span class="co"># cannot verify in R without precision of around 210 bits</span></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>s4 <span class="ot">&lt;-</span> <span class="fu">sample.split</span>(Parole<span class="sc">$</span>Violator, <span class="at">SplitRatio =</span> <span class="fl">0.7</span>)</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>tr4 <span class="ot">&lt;-</span> <span class="fu">subset</span>(Parole, s4 <span class="sc">==</span> <span class="cn">TRUE</span>)</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>te4 <span class="ot">&lt;-</span> <span class="fu">subset</span>(Parole, s4 <span class="sc">==</span> <span class="cn">FALSE</span>)</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a><span class="fu">identical</span>(s1, s4, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## [1] FALSE</code></pre>
</div>
<div id="e-1" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> (e)</h2>
<p>Q: If you tested other training/testing set splits in the previous
section, please re-run the original 5 lines of code to obtain the
original split. Using <code>glm</code>, train a logistic regression
model on the training set. Your dependent variable is
<code>Violator</code>, and you should use all the other variables as
independent variables. What variables are significant in this model?
Significant variables should have a least one star, or should have a
p-value less than 0.05</p>
<p>A:</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">144</span>)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caTools)  <span class="co"># not required by this point</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>split <span class="ot">&lt;-</span> <span class="fu">sample.split</span>(Parole<span class="sc">$</span>Violator, <span class="at">SplitRatio =</span> <span class="fl">0.7</span>)</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">subset</span>(Parole, split <span class="sc">==</span> <span class="cn">TRUE</span>)</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> <span class="fu">subset</span>(Parole, split <span class="sc">==</span> <span class="cn">FALSE</span>)</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a><span class="co"># keeping the variable names, although preferably label by question</span></span></code></pre></div>
<p>As before we run the <code>glm()</code> function,</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>model_2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(Violator <span class="sc">~</span> ., <span class="at">data =</span> train, <span class="at">family =</span> binomial)</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="co">#summary(model_2)</span></span></code></pre></div>
<p>But we simply seek to get the significant variables:</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>coef_table_2 <span class="ot">&lt;-</span> <span class="fu">summary</span>(model_2)<span class="sc">$</span>coefficients <span class="co">#save the coefficients for later use</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>p_val_2_e <span class="ot">&lt;-</span> coef_table_2[,<span class="dv">4</span>]</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>sig_vars_2 <span class="ot">&lt;-</span> <span class="fu">names</span>(p_val_2_e[p_val_2_e <span class="sc">&lt;=</span> <span class="fl">0.05</span>])</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>sig_vars_2</span></code></pre></div>
<pre><code>## [1] &quot;RaceWhite&quot;        &quot;StateVirginia&quot;    &quot;MultipleOffenses&quot;</code></pre>
<p>The significant variables at the 5% significance level are
<code>RaceWhite</code>, <code>StateVirginia</code> and
<code>MultipleOffenses</code>.</p>
</div>
<div id="f-1" class="section level2" number="2.6">
<h2><span class="header-section-number">2.6</span> (f)</h2>
<p>Q: What can we say based on the coefficient of the
<code>MultipleOffenses</code> variable?</p>
<ul>
<li>Our model predicts that parolees who committed multiple offenses
have 1.61 times higher odds of being a violator than the average
parolee.</li>
<li>Our model predicts that a parolee who committed multiple offenses
has 1.61 times higher odds of being a violator than a parolee who did
not commit multiple offenses but is otherwise identical.</li>
<li>Our model predicts that parolees who committed multiple offenses
have 5.01 times higher odds of being a violator than the average
parolee.</li>
<li>Our model predicts that a parolee who committed multiple offenses
has 5.01 times higher odds of being a violator than a parolee who did
not commit multiple offenses but is otherwise identical.</li>
</ul>
<p>A:</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the corresponding coefficient of the &#39;MultipleOffenses&#39; variable (amount of increase in log odds if one comitted multiple offenses)</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Use row name to extract row (check spelling!); column 1 corresponds to coefficient column</span></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>coef_multiple <span class="ot">&lt;-</span> coef_table_2[<span class="st">&quot;MultipleOffenses&quot;</span>, <span class="dv">1</span>]</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(coef_multiple) <span class="co"># odds</span></span></code></pre></div>
<pre><code>## [1] 5.012786</code></pre>
<p>The binary variable <code>MultipleOffenses</code> takes values 0 if a
person did not commit multiple offenses, else 1. The increase in log
odds is 1.61*1 = 1.61 if the person commited multiple offenses.</p>
<p>This means that the odds is equal to 5.01, and represents the odds of
a parolee to be a violator with multiple offenses, compared to a person
who did not commit multiple offenses but is otherwise identical.
Statement (4) is the appropriate one.</p>
</div>
<div id="g-1" class="section level2" number="2.7">
<h2><span class="header-section-number">2.7</span> (g)</h2>
<p>Q: Consider a parolee who is male, of white race, aged 50 years at
prison release, from Kentucky, served 3 months, had a maximum sentence
of 12 months, did not commit multiple offenses, and committed a larceny.
According to the model, what are the odds this individual is a violator?
According to the model, what is the probability this individual is a
violator?</p>
<p>A: The log odds of the given individual being a violator is</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Here we prep the coefficients we need</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>coef_2 <span class="ot">&lt;-</span> coef_table_2 <span class="co">#includes many coefficients we don&#39;t need</span></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a><span class="co"># We want to remove the coefficients for StateLouisiana, StateOther, StateVirginia, CrimeDrugs, CrimeOther</span></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>coef_2 <span class="ot">&lt;-</span> coef_2[<span class="sc">!</span>(<span class="fu">startsWith</span>(<span class="fu">rownames</span>(coef_2), <span class="st">&quot;State&quot;</span>)  <span class="sc">|</span></span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>                   <span class="fu">startsWith</span>(<span class="fu">rownames</span>(coef_2), <span class="st">&quot;Crime&quot;</span>)) <span class="sc">|</span></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">rownames</span>(coef_2) <span class="sc">==</span> <span class="st">&quot;CrimeLarceny&quot;</span>,]</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Or we can also choose the variables (rows) we want to keep</span></span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a><span class="co"># keep_var &lt;- c(&quot;(Intercept)&quot;,&quot;Male&quot;, &quot;RaceWhite&quot;, &quot;Age&quot;, &quot;TimeServed&quot;, &quot;MaxSentence&quot;, &quot;MultipleOffenses&quot;, &quot;CrimeLarceny&quot;)</span></span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a><span class="co"># coef_2 &lt;- coef_table_2[keep_var,]</span></span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that we retain CrimeLarceny because our prisoner has that</span></span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Also note that StateKentucky is being used as a reference</span></span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a><span class="co"># and hence there is no coefficient</span></span></code></pre></div>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Based on information given, define x_2</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>x_2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,  <span class="co"># intercept</span></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>         <span class="dv">1</span>,  <span class="co"># male</span></span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>         <span class="dv">1</span>,  <span class="co"># white</span></span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>         <span class="dv">50</span>, <span class="co"># age</span></span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>         <span class="dv">3</span>,  <span class="co"># time served</span></span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>         <span class="dv">12</span>, <span class="co"># max sentence</span></span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>         <span class="dv">0</span>,  <span class="co"># multiple offenses</span></span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>         <span class="dv">1</span>)  <span class="co"># larceny</span></span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>logodds_2 <span class="ot">&lt;-</span> coef_2[,<span class="dv">1</span>] <span class="sc">%*%</span> x_2  <span class="co"># matrix mult</span></span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a><span class="co">#logodds_2</span></span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a><span class="co">#exp(logodds_2)</span></span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a>prob <span class="ot">=</span> <span class="fu">exp</span>(logodds_2)<span class="sc">/</span> (<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(logodds_2))</span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a>prob</span></code></pre></div>
<pre><code>##          [,1]
## [1,] 0.221434</code></pre>
<p>The probability that the person with the given attributes is a
violator, according to the model is 0.221.</p>
</div>
<div id="h-1" class="section level2" number="2.8">
<h2><span class="header-section-number">2.8</span> (h)</h2>
<p>Q: Use the <code>predict()</code> function to obtain the model’s
predicted probabilities for parolees in the test set. What is the
maximum predicted probability of a violation?</p>
<p>A:</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use test data obtained from subsetting in (d)</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>pred_2_h <span class="ot">&lt;-</span> <span class="fu">predict</span>(model_2, <span class="at">newdata =</span> test, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="fu">max</span>(pred_2_h)</span></code></pre></div>
<pre><code>## [1] 0.9072791</code></pre>
<p>The maximum predicted probability of violation is 0.907.</p>
</div>
<div id="i" class="section level2" number="2.9">
<h2><span class="header-section-number">2.9</span> (i)</h2>
<p>Q: In the following questions, evaluate the model’s predictions on
the test set using a threshold of 0.5. What is the model’s sensitivity?
What is the model’s specificity? What is the model’s accuracy?</p>
<p>A:</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>pred_table_2 <span class="ot">&lt;-</span> <span class="fu">table</span>((pred_2_h <span class="sc">&gt;</span> <span class="fl">0.5</span>), test<span class="sc">$</span>Violator)</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>pred_table_2</span></code></pre></div>
<pre><code>##        
##           0   1
##   FALSE 167  11
##   TRUE   12  12</code></pre>
<p>The answer lies in the table above. Calculating with R directly:</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sensitivity (true positive rate)</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>pred_table_2[<span class="dv">2</span>,<span class="dv">2</span>]<span class="sc">/</span><span class="fu">sum</span>(pred_table_2[,<span class="dv">2</span>])</span></code></pre></div>
<pre><code>## [1] 0.5217391</code></pre>
<p>The Sensitivity (True Positive Rate) is 12/(11+12)=0.521</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="co"># specificity (true negative rate)</span></span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>pred_table_2[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span><span class="fu">sum</span>(pred_table_2[,<span class="dv">1</span>])</span></code></pre></div>
<pre><code>## [1] 0.9329609</code></pre>
<p>The Specificity (True Negative Rate) is 167/(167+12)=0.932</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="co"># accuracy (accuracy)</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">diag</span>(pred_table_2)<span class="sc">/</span><span class="fu">sum</span>(pred_table_2))</span></code></pre></div>
<pre><code>## [1] 0.8861386</code></pre>
<p>The Accuracy is (167+12)/(167+11+12+12)=0.886</p>
</div>
<div id="j" class="section level2" number="2.10">
<h2><span class="header-section-number">2.10</span> (j)</h2>
<p>Q: What is the accuracy of a simple model that predicts that every
parolee is a non-violator?</p>
<p>A:</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># To predict every parolee as a non-violator, model will have accuracy = total number of non-violators/total number of parolees</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(test<span class="sc">$</span>Violator)[<span class="dv">1</span>]<span class="sc">/</span><span class="fu">nrow</span>(test)</span></code></pre></div>
<pre><code>##         0 
## 0.8861386</code></pre>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="co"># or anything that counts the 0s in the column</span></span></code></pre></div>
<p>The accuracy of a simple model that predicts that every parolee is a
non-violator is 179/202=0.886.</p>
</div>
<div id="k" class="section level2" number="2.11">
<h2><span class="header-section-number">2.11</span> (k)</h2>
<p>Q: Consider a parole board using the model to predict whether
parolees will be violators or not. The job of a parole board is to make
sure that a prisoner is ready to be released into free society, and
therefore parole boards tend to be particularily concerned with
releasing prisoners who will violate their parole. Which of the
following most likely describes their preferences and best course of
action?</p>
<ul>
<li>The board assigns more cost to a false negative than a false
positive, and should therefore use a logistic regression cutoff higher
than 0.5.</li>
<li>The board assigns more cost to a false negative than a false
positive, and should therefore use a logistic regression cutoff less
than 0.5.</li>
<li>The board assigns equal cost to a false positive and a false
negative, and should therefore use a logistic regression cutoff equal to
0.5.</li>
<li>The board assigns more cost to a false positive than a false
negative, and should therefore use a logistic regression cutoff higher
than 0.5.</li>
<li>The board assigns more cost to a false positive than a false
negative, and should therefore use a logistic regression cutoff less
than 0.5</li>
</ul>
<p>A: The answer is the second option. Clearly, in this context, false
negatives are a worry where parolees who will be violators are released.
Thus, it is natural for the board to assign more cost to false negatives
than false positives, and should use a cutoff less than 0.5. Lowering
the cutoff makes the model predict more people to be positive, thus
reducing this undesirable outcome.</p>
</div>
<div id="l" class="section level2" number="2.12">
<h2><span class="header-section-number">2.12</span> (l)</h2>
<p>Q: Which of the following is the most accurate assessment of the
value of the logistic regression model with a cutoff 0.5 to a parole
board, based on the model’s accuracy as compared to the simple baseline
model?</p>
<ul>
<li>The model is of limited value to the board because it cannot
outperform a simple baseline, and using a different logistic regression
cutoff is unlikely to improve the model’s value.</li>
<li>The model is of limited value to the board because it cannot
outperform a simple baseline, and using a different logistic regression
cutoff is likely to improve the model’s value.</li>
<li>The model is likely of value to the board, and using a different
logistic regression cutoff is unlikely to improve the model’s
value.</li>
<li>The model is likely of value to the board, and using a different
logistic regression cutoff is likely to improve the model’s value.</li>
</ul>
<p>A: The model is of likely value to the board since it can provide a
better characterisation than the simple model. While both models have
the same accuracy, the baseline model produces many false negatives (23)
compared to (11). Changing the threshold is likely to improve the
model’s value. Thus, the last option is the most accurate
assessment.</p>
</div>
<div id="m" class="section level2" number="2.13">
<h2><span class="header-section-number">2.13</span> (m)</h2>
<p>Q: Using the <code>ROCR</code> package, what is the AUC value for the
model?</p>
<p>A:</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressMessages</span>(<span class="fu">library</span>(ROCR))  <span class="co"># suppressMessages not critical - this is used as loading this library prints dependencies loaded</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Make a prediction and find the model performance using predicted values</span></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>predrocr_2 <span class="ot">&lt;-</span> <span class="fu">prediction</span>(pred_2_h, test<span class="sc">$</span>Violator)</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>auc_2 <span class="ot">&lt;-</span> <span class="fu">performance</span>(predrocr_2, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)<span class="sc">@</span>y.values[[<span class="dv">1</span>]]</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>auc_2 <span class="co"># AUC value (area under curve) - the closer to 1, the better</span></span></code></pre></div>
<pre><code>## [1] 0.8945834</code></pre>
<p>The AUC value for the model is 0.895.</p>
</div>
<div id="n" class="section level2" number="2.14">
<h2><span class="header-section-number">2.14</span> (n)</h2>
<p>Q: Describe the meaning of AUC in this context.</p>
<ul>
<li>The probability the model can correctly differentiate between a
randomly selected parole violator and a randomly selected parole
non-violator.</li>
<li>The model’s accuracy at logistic regression cutoff of 0.5.</li>
<li>The model’s accuracy at the logistic regression cutoff at which it
is most accurate.</li>
</ul>
<p>A: The AUC can be interpreted as the probability that the model can
correctly differentate between a randomly-selected parole violator, and
a randomly-selected parole non-violator.</p>
</div>
<div id="o" class="section level2" number="2.15">
<h2><span class="header-section-number">2.15</span> (o)</h2>
<p>Q: Our goal has been to predict the outcome of a parole decision, and
we used a publicly available dataset of parole releases for predictions.
In this final problem, we will evaluate a potential source of bias
associated with our analysis. It is always important to evaluate a
dataset for possible sources of bias. The dataset contains all
individuals released from parole in 2004, either due to completing their
parole term or violating the terms of their parole. However, it does not
contain parolees who neither violated their parole nor completed their
term in 2004, causing non-violators to be underrepresented. This is
called “selection bias” or “selecting on the dependent variable,”
because only a subset of all relevant parolees were included in our
analysis, based on our dependent variable in this analysis (parole
violation). How could we improve our dataset to best address selection
bias?</p>
<ul>
<li>There is no way to address this form of biasing.</li>
<li>We should use the current dataset, expanded to include the missing
parolees. Each added parolee should be labeled with Violator=0, because
they have not yet had a violation.</li>
<li>We should use the current dataset, expanded to include the missing
parolees. Each added parolee should be labeled with Violator=NA, because
the true outcome has not been observed for these individuals.</li>
<li>We should use a dataset tracking a group of parolees from the start
of their parole until either they violated parole or they completed
their term</li>
</ul>
<p>A: Option 2 does not capture the true outcome of parolees since they
are still either in jail, or not violated thus far. Option 3 does not
help us to build a better model. Option 4 is the best, where they are
tracked until they violate the parole or complete the term. However,
such a dataset requires more effort to gather.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="fu">setdiff</span>(<span class="fu">ls</span>(), <span class="fu">ls</span>(<span class="at">pattern =</span> <span class="st">&quot;SETUP&quot;</span>))</span></code></pre></div>
<pre><code>##  [1] &quot;auc_2&quot;         &quot;coef_2&quot;        &quot;coef_multiple&quot; &quot;coef_table_2&quot; 
##  [5] &quot;logodds_2&quot;     &quot;model_2&quot;       &quot;p_val_2_e&quot;     &quot;Parole&quot;       
##  [9] &quot;pred_2_h&quot;      &quot;pred_table_2&quot;  &quot;predrocr_2&quot;    &quot;prob&quot;         
## [13] &quot;s1&quot;            &quot;s2&quot;            &quot;s3&quot;            &quot;s4&quot;           
## [17] &quot;sig_vars_2&quot;    &quot;split&quot;         &quot;te1&quot;           &quot;te2&quot;          
## [21] &quot;te3&quot;           &quot;te4&quot;           &quot;test&quot;          &quot;tr1&quot;          
## [25] &quot;tr2&quot;           &quot;tr3&quot;           &quot;tr4&quot;           &quot;train&quot;        
## [29] &quot;x_2&quot;</code></pre>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">setdiff</span>(<span class="fu">ls</span>(), <span class="fu">ls</span>(<span class="at">pattern =</span> <span class="st">&quot;SETUP&quot;</span>)))</span></code></pre></div>
</div>
</div>
<div id="question-3" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Question 3</h1>
<div id="a-2" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> (a)</h2>
<p>Q: Read the data into the dataframe <code>germancredit</code>. We are
interested in predicting the <code>resp</code> variable. Obtain a random
training/test set split with:</p>
<p><code>&gt; set.seed(2019)</code> <code>&gt; library(caTools)</code>
<code>&gt; spl &lt;- sample.split(germancredit$resp, 0.75)</code></p>
<p>Split the data frame into a training data frame called “training”
using the observations for which <code>spl</code> is TRUE and a test
data frame called “test” using the observations for which
<code>spl</code> is FALSE. Why do we use the <code>sample.split()</code>
function to split into a training and testing set?</p>
<ul>
<li>It is the only method in R to randomly split the data.</li>
<li>It balances the independent variables between the training and
testing sets.</li>
<li>It balances the dependent variable between the training and testing
sets.</li>
</ul>
<p>Select the best option.</p>
<p>A:</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>germancredit <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;germancredit.csv&quot;</span>)</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a><span class="co"># str(germancredit)</span></span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2019</span>)</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caTools)</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>spl <span class="ot">&lt;-</span> <span class="fu">sample.split</span>(germancredit<span class="sc">$</span>resp, <span class="fl">0.75</span>)</span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>training <span class="ot">&lt;-</span> <span class="fu">subset</span>(germancredit,spl <span class="sc">==</span> <span class="cn">TRUE</span>)</span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> <span class="fu">subset</span>(germancredit,spl <span class="sc">==</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>(Option 3) The reason for splitting the dataset in such a way is to
ensure that the dependent variable is balanced between the training and
test sets.</p>
</div>
<div id="b-2" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> (b)</h2>
<p>Q: We start with the simplest logistic regression model to predict
credit risk in the training set using no predictor variables except the
constant (intercept). Write down the fitted model.</p>
<p>A:</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>model_3_b <span class="ot">&lt;-</span> <span class="fu">glm</span>(resp <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> training, <span class="at">family =</span> binomial)</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> model_3_b<span class="sc">$</span>coefficients[<span class="dv">1</span>]  </span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>x <span class="co"># intercept</span></span></code></pre></div>
<pre><code>## (Intercept) 
##   0.8472979</code></pre>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(x)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(x)) <span class="co">#probability</span></span></code></pre></div>
<pre><code>## (Intercept) 
##         0.7</code></pre>
<p>The intercept is 0.847 Probability(resp=1) =
e<sup>0.847/(1+e</sup>0.847) = 0.7</p>
</div>
<div id="c-2" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> (c)</h2>
<p>Q: Provide a precise mathematical relationship between the estimated
coefficient and the fraction of respondents with a good credit rating in
the training set.</p>
<p>A:</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Variable &#39;resp&#39; gives the credit rating of the respondents</span></span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>n_good <span class="ot">&lt;-</span> <span class="fu">table</span>(training<span class="sc">$</span>resp)[<span class="st">&quot;1&quot;</span>]</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>n_bad <span class="ot">&lt;-</span> <span class="fu">table</span>(training<span class="sc">$</span>resp)[<span class="st">&quot;0&quot;</span>]</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a>n_good<span class="sc">/</span>(n_good<span class="sc">+</span>n_bad) <span class="co"># same as probability of resp=1 in simple logistic regression model in (b)</span></span></code></pre></div>
<pre><code>##   1 
## 0.7</code></pre>
<p>The result in part <a href="#b-2">(b)</a> is exactly equal to the
fraction of the number of people in the training set with a good credit
rating.</p>
</div>
<div id="d-2" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> (d)</h2>
<p>Q: We now develop a logistic regression model to predict credit card
default using all the possible predictor variables. Identify all
variables that are significant at the 10% level.</p>
<p>A:</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>model_3_d <span class="ot">&lt;-</span> <span class="fu">glm</span>(resp <span class="sc">~</span> ., <span class="at">data =</span> training, <span class="at">family =</span> binomial)</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a><span class="co">#summary(model_3_d)</span></span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>p_val_3 <span class="ot">&lt;-</span> <span class="fu">summary</span>(model_3_d)<span class="sc">$</span>coefficients[,<span class="dv">4</span>] <span class="co">#p values</span></span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>sig_vars_3 <span class="ot">&lt;-</span> <span class="fu">names</span>(p_val_3[p_val_3 <span class="sc">&lt;=</span> <span class="fl">0.10</span>]) <span class="co">#10% level</span></span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>sig_vars_3</span></code></pre></div>
<pre><code>##  [1] &quot;chkacct&quot;    &quot;hist&quot;       &quot;newcar&quot;     &quot;amt&quot;        &quot;sav&quot;       
##  [6] &quot;emp&quot;        &quot;instrate&quot;   &quot;malesingle&quot; &quot;guar&quot;       &quot;other&quot;     
## [11] &quot;for.&quot;</code></pre>
<p>The significant variables are: “chkacct”, “hist”, “newcar”, “amt”,
“sav”, “emp”, “instrate”, “malesingle”, “guar”, “other”, “for.”</p>
</div>
<div id="e-2" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> (e)</h2>
<p>Q: What is the log likelihood value for this model?</p>
<p>A: We can calculate log likelihood from the deviance:</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span>model_3_d<span class="sc">$</span>deviance<span class="sc">/</span><span class="dv">2</span></span></code></pre></div>
<pre><code>## [1] -342.8693</code></pre>
<p>Or get it using <code>logLik()</code></p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="fu">logLik</span>(model_3_d)</span></code></pre></div>
<pre><code>## &#39;log Lik.&#39; -342.8693 (df=31)</code></pre>
<p>Hence, the log likelihood of the estimated value is -342.86</p>
</div>
<div id="f-2" class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> (f)</h2>
<p>Q: Compute the confusion matrix on the test set. For the logistic
regression model use a threshold of 0.5.</p>
<p>A:</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="co"># using the suffix 3_d because (d) will be the reference to this model</span></span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a>pred_3_d <span class="ot">&lt;-</span> <span class="fu">predict</span>(model_3_d, <span class="at">newdata =</span> test, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a><span class="co"># at threshold 0.5</span></span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(pred_3_d <span class="sc">&gt;=</span> <span class="fl">0.5</span>, test<span class="sc">$</span>resp)</span></code></pre></div>
<pre><code>##        
##           0   1
##   FALSE  44  22
##   TRUE   31 153</code></pre>
<p>This leads us to the confusion matrix:</p>
<table>
<colgroup>
<col width="39%" />
<col width="29%" />
<col width="30%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Actual Bad Credit Risk</th>
<th>Actual Good Credit Risk</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Predicted Bad Credit Risk</strong></td>
<td>44</td>
<td>22</td>
</tr>
<tr class="even">
<td><strong>Predicted Good Credit Risk</strong></td>
<td>31</td>
<td>153</td>
</tr>
</tbody>
</table>
</div>
<div id="g-2" class="section level2" number="3.7">
<h2><span class="header-section-number">3.7</span> (g)</h2>
<p>Q: What is the accuracy of the model?</p>
<p>A:</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">diag</span>(<span class="fu">table</span>(pred_3_d <span class="sc">&gt;=</span> <span class="fl">0.5</span>, test<span class="sc">$</span>resp)))<span class="sc">/</span><span class="fu">nrow</span>(test)  </span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>x</span></code></pre></div>
<pre><code>## [1] 0.788</code></pre>
<p>The accuracy of the model is 0.788.</p>
</div>
<div id="h-2" class="section level2" number="3.8">
<h2><span class="header-section-number">3.8</span> (h)</h2>
<p>Q: Redo the logistic regression model to predict credit risk using
only the predictor variables that were significant at the 10% level in
<a href="#d-2">(d)</a>. What is the AIC value for this model?</p>
<p>A:</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="co"># using predictor vairables significant at 10% level</span></span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>model_3_h <span class="ot">&lt;-</span> <span class="fu">glm</span>(<span class="fu">as.formula</span>(</span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">paste0</span>(<span class="st">&quot;resp ~ &quot;</span>,</span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a>           <span class="fu">paste0</span>(sig_vars_3, <span class="at">collapse =</span> <span class="st">&quot; + &quot;</span>),</span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true" tabindex="-1"></a>           <span class="st">&quot;- 1&quot;</span>)  <span class="co"># intercept not significant at 3(d)</span></span>
<span id="cb111-6"><a href="#cb111-6" aria-hidden="true" tabindex="-1"></a>           ) , <span class="at">data =</span> training, <span class="at">family =</span> binomial)</span>
<span id="cb111-7"><a href="#cb111-7" aria-hidden="true" tabindex="-1"></a>model_3_h<span class="sc">$</span>aic <span class="co"># aic - the lower the better</span></span></code></pre></div>
<pre><code>## [1] 737.9756</code></pre>
<p>The AIC value for this model is 737.97.</p>
</div>
<div id="i-1" class="section level2" number="3.9">
<h2><span class="header-section-number">3.9</span> (i)</h2>
<p>Q: Based on the AIC, which model ((d) or (h)) is preferable?</p>
<p>A:</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a>model_3_d<span class="sc">$</span>aic  <span class="co"># model from (d)</span></span></code></pre></div>
<pre><code>## [1] 747.7385</code></pre>
<p>AIC of model (d) is 747.73. As the AIC of the model in (h) is better
(lower), model (h) is preferable.</p>
</div>
<div id="j-1" class="section level2" number="3.10">
<h2><span class="header-section-number">3.10</span> (j)</h2>
<p>Q: Compute the confusion matrix on the test set for the model in <a href="#h-2">(h)</a>. For the logistic regression model use a threshold
of 0.5.</p>
<p>A:</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="co"># similarly, the reference for this model is (h)</span></span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a>pred_3_h <span class="ot">&lt;-</span> <span class="fu">predict</span>(model_3_h, <span class="at">newdata =</span> test, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a><span class="co"># at threshold 0.5</span></span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(pred_3_h <span class="sc">&gt;=</span> <span class="fl">0.5</span>, test<span class="sc">$</span>resp)</span></code></pre></div>
<pre><code>##        
##           0   1
##   FALSE  40  21
##   TRUE   35 154</code></pre>
<p>This gives us the following confusion matrix:</p>
<table>
<colgroup>
<col width="39%" />
<col width="29%" />
<col width="30%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Actual Bad Credit Risk</th>
<th>Actual Good Credit Risk</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Predicted Bad Credit Risk</strong></td>
<td>40</td>
<td>21</td>
</tr>
<tr class="even">
<td><strong>Predicted Good Credit Risk</strong></td>
<td>35</td>
<td>154</td>
</tr>
</tbody>
</table>
</div>
<div id="k-1" class="section level2" number="3.11">
<h2><span class="header-section-number">3.11</span> (k)</h2>
<p>Q: Based on the fraction of people who are predicted as good credit
risk but are actually bad credit risk in the test set, which model is
preferable?</p>
<p>A: This question is looking for the model that minimizes Type I
errors or false positives. We can compare the false positive rate of
both models directly:</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a>pred_table_3_d <span class="ot">&lt;-</span> <span class="fu">table</span>(pred_3_d <span class="sc">&gt;=</span> <span class="fl">0.5</span>, test<span class="sc">$</span>resp)</span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a>pred_table_3_h <span class="ot">&lt;-</span> <span class="fu">table</span>(pred_3_h <span class="sc">&gt;=</span> <span class="fl">0.5</span>, test<span class="sc">$</span>resp)</span>
<span id="cb117-3"><a href="#cb117-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-4"><a href="#cb117-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Tabulate false positive rates for each model</span></span>
<span id="cb117-5"><a href="#cb117-5" aria-hidden="true" tabindex="-1"></a>pred_table_3_d[<span class="dv">2</span>,<span class="dv">1</span>]<span class="sc">/</span><span class="fu">sum</span>(pred_table_3_d[,<span class="dv">1</span>]) <span class="co">#fpr for model d</span></span>
<span id="cb117-6"><a href="#cb117-6" aria-hidden="true" tabindex="-1"></a>pred_table_3_h[<span class="dv">2</span>,<span class="dv">1</span>]<span class="sc">/</span><span class="fu">sum</span>(pred_table_3_h[,<span class="dv">1</span>]) <span class="co">#fpr for model h</span></span></code></pre></div>
<pre><code>## [1] 0.4133333
## [1] 0.4666667</code></pre>
<p>Model (d) has a false positive rate of 0.413, while model (h) has a
false positive rate of 0.467 Hence, the model in (d) has lower Type I
error rate, so it is better.</p>
</div>
<div id="l-1" class="section level2" number="3.12">
<h2><span class="header-section-number">3.12</span> (l)</h2>
<p>Q: Based on the fraction of people who are predicted as bad credit
risk but are actually good credit risk in the test set, which model is
preferable?</p>
<p>A: This question is looking for the model that minimizes Type II
errors or false negatives. We can compare the false negative rate of
both models:</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Tabulate false negative rates for each model</span></span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a>pred_table_3_d[<span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">/</span><span class="fu">sum</span>(pred_table_3_d[,<span class="dv">2</span>]) <span class="co">#fnr for model d</span></span></code></pre></div>
<pre><code>## [1] 0.1257143</code></pre>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a>pred_table_3_h[<span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">/</span><span class="fu">sum</span>(pred_table_3_h[,<span class="dv">2</span>]) <span class="co">#fnr for model h</span></span></code></pre></div>
<pre><code>## [1] 0.12</code></pre>
<p>Model (d) has a false negative rate of 0.125, while model (h) has a
false positive rate of 0.12 Hence, the model in (h) has lower Type II
error rate, it is the preferable model in this context.</p>
</div>
<div id="m-1" class="section level2" number="3.13">
<h2><span class="header-section-number">3.13</span> (m)</h2>
<p>Q: Based on the area under the curve in the test set, which model is
preferable?</p>
<p>A:</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ROCR)  <span class="co"># this time no messages as it is already loaded in this document</span></span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a>rocr_3_d <span class="ot">&lt;-</span> <span class="fu">prediction</span>(pred_3_d, test<span class="sc">$</span>resp)</span>
<span id="cb123-3"><a href="#cb123-3" aria-hidden="true" tabindex="-1"></a>auc_3_d <span class="ot">&lt;-</span> <span class="fu">performance</span>(rocr_3_d, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)<span class="sc">@</span>y.values</span>
<span id="cb123-4"><a href="#cb123-4" aria-hidden="true" tabindex="-1"></a>auc_3_d[[<span class="dv">1</span>]] <span class="co">#auc for model d</span></span></code></pre></div>
<pre><code>## [1] 0.8041143</code></pre>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>rocr_3_h <span class="ot">&lt;-</span> <span class="fu">prediction</span>(pred_3_h, test<span class="sc">$</span>resp)</span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a>auc_3_h <span class="ot">&lt;-</span> <span class="fu">performance</span>(rocr_3_h, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)<span class="sc">@</span>y.values</span>
<span id="cb125-3"><a href="#cb125-3" aria-hidden="true" tabindex="-1"></a>auc_3_h[[<span class="dv">1</span>]] <span class="co">#auc for model h</span></span></code></pre></div>
<pre><code>## [1] 0.796419</code></pre>
<p>Model (d) has a AUC of 0.804, while model (h) has a AUC of 0.796
Hence, the model in (d) has higher AUC, so it is the preferred model
under this metric.</p>
</div>
<div id="n-1" class="section level2" number="3.14">
<h2><span class="header-section-number">3.14</span> (n)</h2>
<p>Q: From this point onwards, we use the model with all the predictor
variables included (model (d)). We now consider a more sophisticated way
to evaluate the consequence of misclassification. The consequences of
misclassification by the credit company is assessed as follows: the
costs of incorrectly saying an applicant is a good credit risk is 300 DM
while the profit of correctly saying an applicant is a good credit risk
is 100 DM. In terms of profit this can be considered in terms of a table
as follows:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Actual Bad</th>
<th>Actual Good</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Predicted Bad</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>Predicted Good</td>
<td>-300 DM</td>
<td>100 DM</td>
</tr>
</tbody>
</table>
<p>What is the total profit incurred by the credit company on the test
set?</p>
<p>A: We can solve this with a Hadamard (element-wise) product and sum,
both of which can be done quite readily:</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(pred_table_3_d <span class="sc">*</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="sc">-</span><span class="dv">300</span>,<span class="dv">0</span>,<span class="dv">100</span>), <span class="at">nrow =</span> <span class="dv">2</span>, <span class="at">ncol =</span> <span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 6000</code></pre>
<p>The total profit is 6000 DM.</p>
</div>
<div id="o-1" class="section level2" number="3.15">
<h2><span class="header-section-number">3.15</span> (o)</h2>
<p>Q: To see if we can improve the performance by changing the
threshold, we will use the predicted probability of credit risk from the
logistic regression as a basis by selecting the good credit risks first,
followed by poorer risk applicants. Sort the test set on the predicted
probability of good credit risk from high to low (Hint: You can use the
<code>sort()</code> command). What is the duration of credit in months
for the individual with the lowest predicted probability of good credit
risk?</p>
<p>A:</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort predicted probability from high to low</span></span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a>sort_pred_3_d <span class="ot">&lt;-</span> <span class="fu">sort</span>(pred_3_d, <span class="at">decreasing =</span> <span class="cn">TRUE</span>)</span>
<span id="cb129-3"><a href="#cb129-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-4"><a href="#cb129-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the duration of the last individual</span></span>
<span id="cb129-5"><a href="#cb129-5" aria-hidden="true" tabindex="-1"></a>lowest_prob <span class="ot">&lt;-</span> sort_pred_3_d[<span class="fu">length</span>(sort_pred_3_d)]</span>
<span id="cb129-6"><a href="#cb129-6" aria-hidden="true" tabindex="-1"></a>last_man <span class="ot">&lt;-</span> <span class="fu">names</span>(lowest_prob) <span class="co"># index of row with lowest prob</span></span>
<span id="cb129-7"><a href="#cb129-7" aria-hidden="true" tabindex="-1"></a>germancredit[last_man, ]<span class="sc">$</span>dur <span class="co"># duration of credit</span></span></code></pre></div>
<pre><code>## [1] 48</code></pre>
<p>The duration of credit in months for the individual with the lowest
predicted probability of good credit risk is 48.</p>
</div>
<div id="p" class="section level2" number="3.16">
<h2><span class="header-section-number">3.16</span> (p)</h2>
<p>Q: For each observation in the sorted test set, calculate the actual
profit of providing credit (use the table in <a href="#n-1">(n)</a>).
Compute the net profit by adding a new variable that captures the
cumulative profit. How many far down the test set do you need to go in
order to get the maximum profit? (Hint. You can use the index from the
<code>index.return</code> argument in the <code>sort</code> function and
use the <code>cumsum</code> function)</p>
<p>A:</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a><span class="co"># first we get the sorted predictions</span></span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a><span class="co"># we need index.return to give the indices with respect to pred_3_d</span></span>
<span id="cb131-3"><a href="#cb131-3" aria-hidden="true" tabindex="-1"></a><span class="co"># which derives from the test set</span></span>
<span id="cb131-4"><a href="#cb131-4" aria-hidden="true" tabindex="-1"></a><span class="co"># there might be other ways of doing this, but this is probably the most convenient way</span></span>
<span id="cb131-5"><a href="#cb131-5" aria-hidden="true" tabindex="-1"></a>sort_pred_3_p <span class="ot">&lt;-</span> <span class="fu">sort</span>(pred_3_d, <span class="at">decreasing =</span> <span class="cn">TRUE</span>, <span class="at">index.return =</span> <span class="cn">TRUE</span>)</span>
<span id="cb131-6"><a href="#cb131-6" aria-hidden="true" tabindex="-1"></a>sort_pred_3_p</span></code></pre></div>
<pre><code>## $x
##        160        716        773        152        315        157         62 
## 0.99410083 0.99209305 0.99181824 0.99160886 0.99029453 0.98788356 0.98651687 
##        568        349         56        166        808        528        281 
## 0.98562793 0.98316079 0.98213880 0.98158003 0.98033779 0.97742212 0.97524491 
##        929        779        105        362        551        719        903 
## 0.97215908 0.97006379 0.97000333 0.96620440 0.96519562 0.96297268 0.96263093 
##          9          1        110        474        367        909        137 
## 0.96261608 0.96121350 0.96059590 0.95809167 0.95796191 0.95666206 0.95428855 
##        755         86        191        995        207         39        489 
## 0.95189423 0.95151819 0.95128112 0.94948135 0.94821905 0.94454938 0.94438456 
##        225        958        711        400         52        868        977 
## 0.94414879 0.93899120 0.93894060 0.93772266 0.93654986 0.93352906 0.93240308 
##        242        512        882        961        425         92        904 
## 0.92752738 0.92746925 0.92336854 0.92145036 0.92077534 0.92074488 0.91465159 
##        109        424        964        978        998        881        677 
## 0.91337931 0.91275336 0.91187073 0.90976700 0.90650680 0.90332151 0.90300520 
##        118        498        544        205        560        771        138 
## 0.90107050 0.90043711 0.89853244 0.89365293 0.89222249 0.89080421 0.88991895 
##        913        663        401        769        218        446        839 
## 0.88469240 0.88313203 0.88224207 0.88161167 0.87975166 0.87878683 0.87849931 
##        858        392        229        883        129        346        364 
## 0.87752238 0.87673731 0.87284702 0.86806130 0.86697454 0.86678670 0.86608892 
##        976        251        194        612        733        381        303 
## 0.86424364 0.86203929 0.86000191 0.85905916 0.85858070 0.85854470 0.85712739 
##        165        283        660        894         78        337        124 
## 0.85618871 0.85492419 0.85436655 0.84869318 0.84671427 0.84285731 0.83899948 
##         84        948        800         76        884        718        561 
## 0.83899535 0.83499245 0.83239081 0.83142583 0.83141785 0.83122259 0.82529039 
##        812        406        133        280        140        766        569 
## 0.82215527 0.80581337 0.80476646 0.80449086 0.80296950 0.79921464 0.79855079 
##        791        821        972        777        829        765        844 
## 0.79551084 0.79353750 0.78812210 0.78501951 0.78363861 0.78210663 0.78209627 
##        731        496        703        588        563        331        402 
## 0.77731825 0.77724758 0.77549669 0.77184139 0.76613616 0.76510680 0.75819346 
##        575        558        803        325        582        966        921 
## 0.74802512 0.74589812 0.74483750 0.74352673 0.74074353 0.73418208 0.73300449 
##        408         44        149        684        780        580        965 
## 0.72147015 0.71953996 0.71901975 0.71690849 0.71653674 0.71485083 0.71128191 
##        639        144        623        264        309        628        135 
## 0.70950056 0.70409049 0.70344157 0.70279953 0.70211745 0.69938677 0.69358876 
##         65        691        530        658        583        683        661 
## 0.68746707 0.68607208 0.68268120 0.67686472 0.67640093 0.67487356 0.66190952 
##        101        676        754        384        922        794        589 
## 0.65851328 0.65562390 0.64988591 0.64887840 0.64588014 0.64557108 0.63579394 
##        850        370        732        195        398        248         14 
## 0.62975394 0.62244794 0.62081280 0.61674454 0.61504316 0.60990941 0.60931860 
##        700        618        288         59        522        805        508 
## 0.60878528 0.59669012 0.59191520 0.59147895 0.58748448 0.58604385 0.57431607 
##        338         87        823        587        592        183        304 
## 0.56733620 0.56546289 0.56191955 0.55905799 0.55691750 0.54981428 0.54910424 
##        752        763        742        738        153        968        308 
## 0.54540254 0.54358894 0.53158674 0.52716815 0.52348170 0.52167568 0.51084923 
##        638        457         36        720        635        182          2 
## 0.50217372 0.50177478 0.49957605 0.49132004 0.48861374 0.48750284 0.47995923 
##        705        483        532        488        478        365         32 
## 0.47884476 0.47497604 0.46617567 0.46611191 0.45837005 0.45552469 0.45149818 
##        574        285        920        767        455        463        227 
## 0.44484355 0.43907457 0.43795750 0.43754884 0.43287603 0.41149851 0.40458351 
##        666        243        339        938        649        936         88 
## 0.39896623 0.39120137 0.38952710 0.38624611 0.38391517 0.37211687 0.36370479 
##        669        514        546        584        986        915        859 
## 0.35880293 0.35706370 0.35109575 0.34807802 0.32475227 0.32377027 0.31188885 
##        397        492        476        613        376         18        789 
## 0.31150111 0.30302874 0.30183753 0.29810553 0.29087711 0.28631651 0.28558554 
##        238        164        886        888        213        557        579 
## 0.27724547 0.27278797 0.26646448 0.26516412 0.26277190 0.25622490 0.23569668 
##        275        877        816        447        624        722        321 
## 0.23517233 0.22947603 0.21583819 0.20983469 0.20693915 0.20393575 0.19376572 
##        867        171         10        529         12        790        708 
## 0.17368757 0.17097915 0.16717073 0.15371705 0.14589582 0.14147026 0.13784364 
##        597        728        833        335        539 
## 0.11624561 0.10156005 0.09794884 0.09687667 0.05141724 
## 
## $ix
##   [1]  41 173 193  38  74  40  15 133  83  13  44 204 120  66 234 195  25  84
##  [19] 127 175 226   3   1  27 107  87 228  33 186  20  48 249  52  10 112  55
##  [37] 238 172  95  12 217 246  59 117 220 239 101  23 227  26 100 240 247 250
##  [55] 219 164  28 115 125  51 130 192  34 229 160  96 191  54 102 211 214  92
##  [73]  57 221  30  82  85 245  62  49 147 181  90  70  43  67 158 225  18  79
##  [91]  29  19 237 201  17 222 174 131 205  98  31  65  35 189 134 199 207 244
## [109] 194 209 188 212 179 114 169 143 132  77  97 136 129 202  76 139 242 232
## [127]  99  11  37 166 196 138 241 155  36 150  63  73 152  32  16 167 122 157
## [145] 140 165 159  24 163 185  91 233 200 144 213  88 180  50  94  61   6 168
## [163] 149  69  14 119 203 116  80  21 208 142 145  47  71 184 187 183 182  39
## [181] 243  72 154 105   9 176 153  46   2 170 110 123 111 109  86   8 135  68
## [199] 231 190 104 106  56 161  60  81 236 156 235  22 162 118 126 141 248 230
## [217] 215  93 113 108 148  89   7 197  58  42 223 224  53 128 137  64 218 206
## [235] 103 151 177  75 216  45   4 121   5 198 171 146 178 210  78 124</code></pre>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="co"># then we get the profit using the sorted test set</span></span>
<span id="cb133-2"><a href="#cb133-2" aria-hidden="true" tabindex="-1"></a><span class="co"># if the model is working reasonably, then we should see</span></span>
<span id="cb133-3"><a href="#cb133-3" aria-hidden="true" tabindex="-1"></a><span class="co"># many 100 (good predictions) nearer the start of this profit vector</span></span>
<span id="cb133-4"><a href="#cb133-4" aria-hidden="true" tabindex="-1"></a>profit_pred_3 <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(test<span class="sc">$</span>resp[sort_pred_3_p<span class="sc">$</span>ix], <span class="dv">100</span>, <span class="sc">-</span><span class="dv">300</span>)</span>
<span id="cb133-5"><a href="#cb133-5" aria-hidden="true" tabindex="-1"></a>profit_pred_3</span></code></pre></div>
<pre><code>##   [1]  100  100  100  100  100  100  100  100  100  100  100  100  100  100  100
##  [16]  100  100  100  100  100  100  100  100  100  100  100  100  100 -300  100
##  [31] -300  100  100  100  100  100  100  100  100  100  100  100  100  100  100
##  [46]  100 -300  100  100  100  100 -300  100  100  100  100  100  100 -300  100
##  [61] -300  100 -300  100  100  100  100  100  100  100  100  100 -300  100  100
##  [76]  100  100  100  100  100 -300  100  100 -300  100  100  100  100  100  100
##  [91]  100  100  100  100  100  100  100  100  100 -300  100  100  100  100  100
## [106] -300  100  100  100 -300  100  100  100 -300  100  100  100  100  100  100
## [121] -300  100  100  100  100  100  100  100  100  100  100  100  100  100 -300
## [136] -300  100 -300 -300  100  100  100  100  100  100  100  100  100  100  100
## [151]  100  100  100 -300 -300  100 -300 -300  100  100 -300  100  100  100  100
## [166] -300  100 -300 -300  100 -300  100  100 -300  100 -300  100  100  100  100
## [181]  100 -300  100  100 -300  100 -300 -300 -300  100  100 -300  100  100 -300
## [196]  100  100  100 -300 -300 -300  100 -300  100 -300  100  100 -300 -300 -300
## [211] -300  100 -300 -300  100 -300 -300  100 -300 -300  100 -300  100 -300 -300
## [226]  100 -300 -300 -300 -300 -300 -300  100  100 -300  100 -300 -300  100 -300
## [241] -300 -300 -300 -300 -300 -300 -300 -300 -300 -300</code></pre>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a><span class="co"># then we get the cumulative profit</span></span>
<span id="cb135-2"><a href="#cb135-2" aria-hidden="true" tabindex="-1"></a><span class="co"># again, assuming the model is working, the prediction errors will</span></span>
<span id="cb135-3"><a href="#cb135-3" aria-hidden="true" tabindex="-1"></a><span class="co"># occur later (i.e. lower predicted probabilities</span></span>
<span id="cb135-4"><a href="#cb135-4" aria-hidden="true" tabindex="-1"></a><span class="co"># means lower actual probabilities in the test set)</span></span>
<span id="cb135-5"><a href="#cb135-5" aria-hidden="true" tabindex="-1"></a><span class="co"># hence we will notice that the cumulative sum gets negative later on</span></span>
<span id="cb135-6"><a href="#cb135-6" aria-hidden="true" tabindex="-1"></a>cumulative_profit_3 <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(profit_pred_3)</span>
<span id="cb135-7"><a href="#cb135-7" aria-hidden="true" tabindex="-1"></a>cumulative_profit_3</span></code></pre></div>
<pre><code>##   [1]   100   200   300   400   500   600   700   800   900  1000  1100  1200
##  [13]  1300  1400  1500  1600  1700  1800  1900  2000  2100  2200  2300  2400
##  [25]  2500  2600  2700  2800  2500  2600  2300  2400  2500  2600  2700  2800
##  [37]  2900  3000  3100  3200  3300  3400  3500  3600  3700  3800  3500  3600
##  [49]  3700  3800  3900  3600  3700  3800  3900  4000  4100  4200  3900  4000
##  [61]  3700  3800  3500  3600  3700  3800  3900  4000  4100  4200  4300  4400
##  [73]  4100  4200  4300  4400  4500  4600  4700  4800  4500  4600  4700  4400
##  [85]  4500  4600  4700  4800  4900  5000  5100  5200  5300  5400  5500  5600
##  [97]  5700  5800  5900  5600  5700  5800  5900  6000  6100  5800  5900  6000
## [109]  6100  5800  5900  6000  6100  5800  5900  6000  6100  6200  6300  6400
## [121]  6100  6200  6300  6400  6500  6600  6700  6800  6900  7000  7100  7200
## [133]  7300  7400  7100  6800  6900  6600  6300  6400  6500  6600  6700  6800
## [145]  6900  7000  7100  7200  7300  7400  7500  7600  7700  7400  7100  7200
## [157]  6900  6600  6700  6800  6500  6600  6700  6800  6900  6600  6700  6400
## [169]  6100  6200  5900  6000  6100  5800  5900  5600  5700  5800  5900  6000
## [181]  6100  5800  5900  6000  5700  5800  5500  5200  4900  5000  5100  4800
## [193]  4900  5000  4700  4800  4900  5000  4700  4400  4100  4200  3900  4000
## [205]  3700  3800  3900  3600  3300  3000  2700  2800  2500  2200  2300  2000
## [217]  1700  1800  1500  1200  1300  1000  1100   800   500   600   300     0
## [229]  -300  -600  -900 -1200 -1100 -1000 -1300 -1200 -1500 -1800 -1700 -2000
## [241] -2300 -2600 -2900 -3200 -3500 -3800 -4100 -4400 -4700 -5000</code></pre>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a><span class="co"># then we simply return what was asked for in the question</span></span>
<span id="cb137-2"><a href="#cb137-2" aria-hidden="true" tabindex="-1"></a><span class="fu">which.max</span>(cumulative_profit_3)</span></code></pre></div>
<pre><code>## [1] 153</code></pre>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a><span class="fu">max</span>(cumulative_profit_3) <span class="co">#the max amount of profits</span></span></code></pre></div>
<pre><code>## [1] 7700</code></pre>
<p>We need to get to the person at the 153 index in the sorted test set
to get the maximum profit (7700).</p>
</div>
<div id="q" class="section level2" number="3.17">
<h2><span class="header-section-number">3.17</span> (q)</h2>
<p>Q: If the logistic regression model from <a href="#p">(p)</a> is
scored to future applicants, what “probability of good credit risk”
cutoff should be used in extending credit?</p>
<p>A: The probability is given by the following code:</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the predicted probability of the individual with index 153 on sorted test set</span></span>
<span id="cb141-2"><a href="#cb141-2" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: individiual with 153 on sorted test set may not be index 153 on unsorted/raw test set</span></span>
<span id="cb141-3"><a href="#cb141-3" aria-hidden="true" tabindex="-1"></a>sort_pred_3_p<span class="sc">$</span>x[<span class="fu">which.max</span>(cumulative_profit_3)] </span></code></pre></div>
<pre><code>##       794 
## 0.6455711</code></pre>
<p>In order to maximise profits, we would use the probability of 0.645.
This serves as a profit-maximising cutoff to credit good and bad risk
based on this data.</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a><span class="co"># NOT RUN</span></span>
<span id="cb143-2"><a href="#cb143-2" aria-hidden="true" tabindex="-1"></a><span class="co"># for testing only (model (h))</span></span>
<span id="cb143-3"><a href="#cb143-3" aria-hidden="true" tabindex="-1"></a>sort_pred_3_test <span class="ot">&lt;-</span> <span class="fu">sort</span>(pred_3_h, <span class="at">decreasing =</span> <span class="cn">TRUE</span>, <span class="at">index.return =</span> <span class="cn">TRUE</span>)</span>
<span id="cb143-4"><a href="#cb143-4" aria-hidden="true" tabindex="-1"></a>profit_pred_3_test <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(test<span class="sc">$</span>resp[sort_pred_3_test<span class="sc">$</span>ix], <span class="dv">100</span>, <span class="sc">-</span><span class="dv">300</span>)</span>
<span id="cb143-5"><a href="#cb143-5" aria-hidden="true" tabindex="-1"></a>cumulative_profit_3_test <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(profit_pred_3_test)</span>
<span id="cb143-6"><a href="#cb143-6" aria-hidden="true" tabindex="-1"></a><span class="fu">max</span>(cumulative_profit_3_test)  <span class="co"># 7900, for seed 2019</span></span></code></pre></div>
<pre><code>## [1] 7900</code></pre>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a><span class="co"># AIC-based models probably still better</span></span></code></pre></div>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="fu">setdiff</span>(<span class="fu">ls</span>(), <span class="fu">ls</span>(<span class="at">pattern =</span> <span class="st">&quot;SETUP&quot;</span>))</span></code></pre></div>
<pre><code>##  [1] &quot;auc_3_d&quot;                  &quot;auc_3_h&quot;                 
##  [3] &quot;cumulative_profit_3&quot;      &quot;cumulative_profit_3_test&quot;
##  [5] &quot;germancredit&quot;             &quot;last_man&quot;                
##  [7] &quot;lowest_prob&quot;              &quot;model_3_b&quot;               
##  [9] &quot;model_3_d&quot;                &quot;model_3_h&quot;               
## [11] &quot;n_bad&quot;                    &quot;n_good&quot;                  
## [13] &quot;p_val_3&quot;                  &quot;pred_3_d&quot;                
## [15] &quot;pred_3_h&quot;                 &quot;pred_table_3_d&quot;          
## [17] &quot;pred_table_3_h&quot;           &quot;profit_pred_3&quot;           
## [19] &quot;profit_pred_3_test&quot;       &quot;rocr_3_d&quot;                
## [21] &quot;rocr_3_h&quot;                 &quot;sig_vars_3&quot;              
## [23] &quot;sort_pred_3_d&quot;            &quot;sort_pred_3_p&quot;           
## [25] &quot;sort_pred_3_test&quot;         &quot;spl&quot;                     
## [27] &quot;test&quot;                     &quot;training&quot;                
## [29] &quot;x&quot;</code></pre>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="#cb148-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">setdiff</span>(<span class="fu">ls</span>(), <span class="fu">ls</span>(<span class="at">pattern =</span> <span class="st">&quot;SETUP&quot;</span>)))</span></code></pre></div>
</div>
</div>
<div id="question-4" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Question 4</h1>
<div id="a-3" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> a)</h2>
<p>Q: Read the dataset into the dataframe <code>pres.</code> In the
elections starting from 1916 up to and including 2016, which party has
won more presidential elections? How many elections has that party
won?</p>
<p>A:</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="#cb149-1" aria-hidden="true" tabindex="-1"></a>pres <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;presidential.csv&quot;</span>)</span>
<span id="cb149-2"><a href="#cb149-2" aria-hidden="true" tabindex="-1"></a><span class="co"># str(pres)</span></span>
<span id="cb149-3"><a href="#cb149-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb149-4"><a href="#cb149-4" aria-hidden="true" tabindex="-1"></a><span class="co"># &#39;WIN&#39; variable shows which party has won</span></span>
<span id="cb149-5"><a href="#cb149-5" aria-hidden="true" tabindex="-1"></a><span class="co"># number of Democratic party elections won under &quot;1&quot;</span></span>
<span id="cb149-6"><a href="#cb149-6" aria-hidden="true" tabindex="-1"></a><span class="co"># number of Republican party elections won under &quot;-1&quot;</span></span>
<span id="cb149-7"><a href="#cb149-7" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(pres<span class="sc">$</span>WIN)[<span class="st">&quot;1&quot;</span>] <span class="sc">&gt;</span> <span class="fu">table</span>(pres<span class="sc">$</span>WIN)[<span class="st">&quot;-1&quot;</span>]</span></code></pre></div>
<pre><code>##     1 
## FALSE</code></pre>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="#cb151-1" aria-hidden="true" tabindex="-1"></a><span class="co"># if TRUE then the Democratic party has won more elections.</span></span>
<span id="cb151-2"><a href="#cb151-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(pres<span class="sc">$</span>WIN)  </span></code></pre></div>
<pre><code>## 
## -1  1 
## 13 13</code></pre>
<p>Both parties have won an equal number of elections, 13 each.</p>
</div>
<div id="b-3" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> b)</h2>
<p>Q:Who among the nominees have represented the Democrats and the
Republicans in the most number of presidential elections? How many times
have they respectively done so?</p>
<p>A:</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="#cb153-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sort</span>(<span class="fu">table</span>(pres<span class="sc">$</span>DEM))</span></code></pre></div>
<pre><code>## 
##       Cox     Davis   Dukakis      Gore H.Clinton  Humphrey   Johnson   Kennedy 
##         1         1         1         1         1         1         1         1 
##     Kerry  McGovern   Mondale     Smith    Truman    Wilson B.Clinton    Carter 
##         1         1         1         1         1         1         2         2 
##     Obama Stevenson Roosevelt 
##         2         2         4</code></pre>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="#cb155-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sort</span>(<span class="fu">table</span>(pres<span class="sc">$</span>REP))</span></code></pre></div>
<pre><code>## 
##   Coolidge       Dole       Ford  Goldwater    Harding     Hughes     Landon 
##          1          1          1          1          1          1          1 
##     McCain     Romney      Trump     Wilkie      Dewey Eisenhower     G.Bush 
##          1          1          1          1          2          2          2 
##   G.W.Bush     Hoover     Reagan      Nixon 
##          2          2          2          3</code></pre>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="#cb157-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(<span class="fu">which.max</span>(<span class="fu">table</span>(pres<span class="sc">$</span>DEM)))</span></code></pre></div>
<pre><code>## [1] &quot;Roosevelt&quot;</code></pre>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="#cb159-1" aria-hidden="true" tabindex="-1"></a><span class="fu">max</span>(<span class="fu">table</span>(pres<span class="sc">$</span>DEM))</span></code></pre></div>
<pre><code>## [1] 4</code></pre>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="#cb161-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(<span class="fu">which.max</span>(<span class="fu">table</span>(pres<span class="sc">$</span>REP)))</span></code></pre></div>
<pre><code>## [1] &quot;Nixon&quot;</code></pre>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="#cb163-1" aria-hidden="true" tabindex="-1"></a><span class="fu">max</span>(<span class="fu">table</span>(pres<span class="sc">$</span>REP))</span></code></pre></div>
<pre><code>## [1] 3</code></pre>
<p>Roosevelt has represented the Democrats the most times out of anyone
in the presidential elections. Roosevelt has represented the Democratic
party 4 times. Nixon has represented the Republicans the most times out
of anyone in the presidential elections. Nixon has represented the
Republican party 3 times.</p>
</div>
<div id="c-3" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> c)</h2>
<p>Q:Use a two-sided t-test to verify if there is evidence to show that
the number of good quarters when the president is Republican is
different from the number of good quarters when the president is
Democratic. What is the p-value of the test and your conclusion?</p>
<p>A:</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="#cb165-1" aria-hidden="true" tabindex="-1"></a><span class="fu">t.test</span>(pres<span class="sc">$</span>GOOD[pres<span class="sc">$</span>INC<span class="sc">==</span><span class="dv">1</span>],pres<span class="sc">$</span>GOOD[pres<span class="sc">$</span>INC<span class="sc">==-</span><span class="dv">1</span>])</span></code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  pres$GOOD[pres$INC == 1] and pres$GOOD[pres$INC == -1]
## t = -0.36727, df = 23.274, p-value = 0.7167
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -2.762023  1.928689
## sample estimates:
## mean of x mean of y 
##  4.500000  4.916667</code></pre>
<p>The p-value for the two sided t-test is 0.7167. There is insufficient
evidence at the 5% significance level to reject the null hypothesis that
the number of good quarters when the president is Democrat or Republican
is the same.</p>
</div>
<div id="d-3" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> d)</h2>
<p>Q:Define a new variable WININC that takes a value of 1 if the
presidential nominee of the incumbent party wins and 0 otherwise.</p>
<p>A:</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="#cb167-1" aria-hidden="true" tabindex="-1"></a>pres<span class="sc">$</span>WININC <span class="ot">&lt;-</span> <span class="fu">as.integer</span>(pres<span class="sc">$</span>INC<span class="sc">==</span>pres<span class="sc">$</span>WIN)</span></code></pre></div>
</div>
<div id="e-3" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> e)</h2>
<p>Q: How many times did the presidential nominee from the incumbent
party win and how many times did the presidential nominee from the
incumbent party lose?</p>
<p>A:</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="#cb168-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(pres<span class="sc">$</span>WININC) <span class="co"># 1 - win, 0 - lose</span></span></code></pre></div>
<pre><code>## 
##  0  1 
## 11 15</code></pre>
<p>The presidential nominee from the incumbent party won 15 times and
lost 11 times.</p>
</div>
<div id="f-3" class="section level2" number="4.6">
<h2><span class="header-section-number">4.6</span> f)</h2>
<p>Q: Perform a simple logistic regression to predict the WININC
variable using the GROWTH variable and the constant. What is the
log-likelihood value for the model?</p>
<p>A:</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="#cb170-1" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">glm</span>(WININC<span class="sc">~</span>GROWTH,<span class="at">data=</span>pres,<span class="at">family=</span>binomial)</span>
<span id="cb170-2"><a href="#cb170-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model1)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = WININC ~ GROWTH, family = binomial, data = pres)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.7182  -1.1494   0.6775   0.8765   1.6541  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -0.08634    0.52064  -0.166   0.8683  
## GROWTH       0.25514    0.14052   1.816   0.0694 .
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 35.426  on 25  degrees of freedom
## Residual deviance: 29.485  on 24  degrees of freedom
## AIC: 33.485
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="#cb172-1" aria-hidden="true" tabindex="-1"></a>llm1<span class="ot">&lt;-</span>(model1<span class="sc">$</span>aic <span class="sc">-</span> <span class="dv">2</span><span class="sc">*</span>model1<span class="sc">$</span>rank)<span class="sc">/</span><span class="dv">2</span></span>
<span id="cb172-2"><a href="#cb172-2" aria-hidden="true" tabindex="-1"></a>llm1 <span class="co">#log likelihood</span></span></code></pre></div>
<pre><code>## [1] 14.74247</code></pre>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="#cb174-1" aria-hidden="true" tabindex="-1"></a><span class="fu">logLik</span>(model1) <span class="co">#log likelihood</span></span></code></pre></div>
<pre><code>## &#39;log Lik.&#39; -14.74247 (df=2)</code></pre>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="#cb176-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model1)<span class="sc">$</span>coefficients[<span class="dv">2</span>,<span class="dv">4</span>] <span class="co">#p-value for GROWTH variable</span></span></code></pre></div>
<pre><code>## [1] 0.06941497</code></pre>
<p>The log likelihood is -14.74</p>
</div>
<div id="g-3" class="section level2" number="4.7">
<h2><span class="header-section-number">4.7</span> g)</h2>
<p>Q: The GROWTH variable is: <em>Significant at the 0.001 level
</em>Significant at the 0.01 level <em>Significant at the 0.05 level
</em>Significant at the 0.1 level *Insignificant</p>
<p>A: The growth variable is significant at level 0.1.</p>
</div>
<div id="h-3" class="section level2" number="4.8">
<h2><span class="header-section-number">4.8</span> h)</h2>
<p>Q: Unlike questions (d) to (g) which looked at the incumbent party’s
winning chances, from this point onwards, we are going to predict the
chances of the Democratic party nominee winning in the presidential
election. To do this, transform the variables as follows: * Transform
the WIN variable to be 1 when the presidential winner is a Democrat and
0 when the winner is a Republican. * Transform the GROWTH variable as
follows: When the growth rate is positive (say 4.623) and the Republican
party is incumbent, we should transform it to a negative value -4.623
since this should have a negative effect on the Democratic nominee’s
chances of winning while if the growth rate is negative (say -4.623) and
the Republican party is incumbent, we should transform it to positive
4.623 since this should have a positive effect on the Democratic
nominee’s chances of winning.</p>
<p>A: We create a new data set <code>presmod</code> with the modified
variables.</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="#cb178-1" aria-hidden="true" tabindex="-1"></a>presmod<span class="ot">&lt;-</span>pres</span>
<span id="cb178-2"><a href="#cb178-2" aria-hidden="true" tabindex="-1"></a>presmod<span class="sc">$</span>WIN <span class="ot">&lt;-</span> <span class="fu">as.integer</span>(pres<span class="sc">$</span>WIN<span class="sc">==</span><span class="dv">1</span>)</span>
<span id="cb178-3"><a href="#cb178-3" aria-hidden="true" tabindex="-1"></a>presmod<span class="sc">$</span>GROWTH <span class="ot">&lt;-</span> pres<span class="sc">$</span>GROWTH<span class="sc">*</span>pres<span class="sc">$</span>INC</span></code></pre></div>
</div>
<div id="i-2" class="section level2" number="4.9">
<h2><span class="header-section-number">4.9</span> i)</h2>
<p>Q: Repeat step ii in question (h) for the GOOD variable. You are now
ready to develop a logistic regression model for the WIN variable using
the predictor variables INC, RUN, DUR, GROWTH, GOOD and the constant
(intercept). Use all the observations to build your model. What is the
AIC of the model?</p>
<p>A:</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="#cb179-1" aria-hidden="true" tabindex="-1"></a>presmod<span class="sc">$</span>GOOD <span class="ot">&lt;-</span> pres<span class="sc">$</span>GOOD<span class="sc">*</span>pres<span class="sc">$</span>INC</span>
<span id="cb179-2"><a href="#cb179-2" aria-hidden="true" tabindex="-1"></a>presmod</span></code></pre></div>
<pre><code>##    YEAR       DEM        REP INC RUN   DUR  GROWTH GOOD WIN WININC
## 1  1916    Wilson     Hughes   1   1  0.00   2.229    3   1      1
## 2  1920       Cox    Harding   1   0  1.00 -11.463    0   0      0
## 3  1924     Davis   Coolidge  -1  -1  0.00   3.872  -10   0      1
## 4  1928     Smith     Hoover  -1   0 -1.00  -4.623   -7   0      1
## 5  1932 Roosevelt     Hoover  -1  -1 -1.25  14.361   -4   1      0
## 6  1936 Roosevelt     Landon   1   1  0.00  11.616    9   1      1
## 7  1940 Roosevelt     Wilkie   1   1  1.00   3.963    8   1      1
## 8  1944 Roosevelt      Dewey   1   1  1.25   4.067    0   1      1
## 9  1948    Truman      Dewey   1   1  1.50   3.348    0   1      1
## 10 1952 Stevenson Eisenhower   1   0  1.75   1.027    7   0      0
## 11 1956 Stevenson Eisenhower  -1  -1  0.00   1.250   -5   0      1
## 12 1960   Kennedy      Nixon  -1   0 -1.00  -0.643   -5   1      0
## 13 1964   Johnson  Goldwater   1   1  0.00   5.099    9   1      1
## 14 1968  Humphrey      Nixon   1   0  1.00   5.107    7   0      0
## 15 1972  McGovern      Nixon  -1  -1  0.00  -5.862   -4   0      1
## 16 1976    Carter       Ford  -1   0 -1.00  -3.828   -5   1      0
## 17 1980    Carter     Reagan   1   1  0.00  -3.596    5   0      0
## 18 1984   Mondale     Reagan  -1  -1  0.00  -5.438   -8   0      1
## 19 1988   Dukakis     G.Bush  -1   0 -1.00  -2.342   -4   0      1
## 20 1992 B.Clinton     G.Bush  -1  -1 -1.25  -3.053   -3   1      0
## 21 1996 B.Clinton       Dole   1   1  0.00   3.300    4   1      1
## 22 2000      Gore   G.W.Bush   1   0  1.00   2.069    7   0      0
## 23 2004     Kerry   G.W.Bush  -1  -1  0.00  -2.118   -2   0      1
## 24 2008     Obama     McCain  -1   0 -1.00   1.701   -2   1      0
## 25 2012     Obama     Romney   1   1  0.00   1.094    2   1      1
## 26 2016 H.Clinton      Trump   1   0  1.00   1.208    2   0      0</code></pre>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="#cb181-1" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(WIN<span class="sc">~</span>INC<span class="sc">+</span>RUN<span class="sc">+</span>GROWTH<span class="sc">+</span>DUR<span class="sc">+</span>GOOD,<span class="at">data=</span>presmod,<span class="at">family=</span>binomial)</span>
<span id="cb181-2"><a href="#cb181-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model2)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = WIN ~ INC + RUN + GROWTH + DUR + GOOD, family = binomial, 
##     data = presmod)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.5903  -0.3333   0.0019   0.2636   2.0473  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -0.71797    0.76816  -0.935   0.3500  
## INC          0.18626    3.39365   0.055   0.9562  
## RUN          3.13145    1.76504   1.774   0.0760 .
## GROWTH       0.71150    0.42379   1.679   0.0932 .
## DUR         -3.23160    2.79487  -1.156   0.2476  
## GOOD        -0.06794    0.30611  -0.222   0.8244  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 36.044  on 25  degrees of freedom
## Residual deviance: 14.755  on 20  degrees of freedom
## AIC: 26.755
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="#cb183-1" aria-hidden="true" tabindex="-1"></a>model2<span class="sc">$</span>aic</span></code></pre></div>
<pre><code>## [1] 26.75462</code></pre>
<p>The AIC for the model is 26.755.</p>
</div>
<div id="j-2" class="section level2" number="4.10">
<h2><span class="header-section-number">4.10</span> j)</h2>
<p>Q: Among the predictor variables INC, RUN, DUR, GROWTH, GOOD and the
constant (intercept), identify the three least significant
variables?</p>
<p>A: Comparing the p-values of the variables in the logistic regression
model summary in <a href="#i-1">(i)</a>, the three least significant
variables are intercept, INC, GOOD.</p>
</div>
<div id="k-2" class="section level2" number="4.11">
<h2><span class="header-section-number">4.11</span> k)</h2>
<p>Q: Drop the three variables identified in question (j) and rebuild
you logistic regression model. What is the AIC of the new model?</p>
<p>A:</p>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="#cb185-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop intercept, INC and GOOD</span></span>
<span id="cb185-2"><a href="#cb185-2" aria-hidden="true" tabindex="-1"></a>model3 <span class="ot">&lt;-</span> <span class="fu">glm</span>(WIN<span class="sc">~</span>RUN<span class="sc">+</span>GROWTH<span class="sc">+</span>DUR<span class="dv">-1</span>, <span class="at">data=</span>presmod, <span class="at">family=</span>binomial)</span>
<span id="cb185-3"><a href="#cb185-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model3)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = WIN ~ RUN + GROWTH + DUR - 1, family = binomial, 
##     data = presmod)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -1.82945  -0.51115   0.00022   0.23856   1.59715  
## 
## Coefficients:
##        Estimate Std. Error z value Pr(&gt;|z|)  
## RUN      2.6997     1.2451   2.168   0.0301 *
## GROWTH   0.6401     0.3753   1.705   0.0881 .
## DUR     -2.9648     1.5814  -1.875   0.0608 .
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 36.044  on 26  degrees of freedom
## Residual deviance: 15.748  on 23  degrees of freedom
## AIC: 21.748
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="#cb187-1" aria-hidden="true" tabindex="-1"></a>model3<span class="sc">$</span>aic</span></code></pre></div>
<pre><code>## [1] 21.74787</code></pre>
<p>The AIC for the new model is 21.748</p>
</div>
<div id="l-2" class="section level2" number="4.12">
<h2><span class="header-section-number">4.12</span> l)</h2>
<p>Q: In this new model, what is the smallest significance level at
which you would reject the null hypothesis that the coefficient for the
variable DUR is zero? Suppose, we now decide to use a level of 0.10,
what would your suggestion be?</p>
<p>A: We will reject the hypothesis <code>DUR=0</code>for any
significance level above 0.0608. Hence at level 0.10, we reject the
hypothesis.</p>
</div>
<div id="m-2" class="section level2" number="4.13">
<h2><span class="header-section-number">4.13</span> m)</h2>
<p>Q: Which among the two models that you have developed in questions
(i) and (k) do you prefer? Explain your reasons briefly.</p>
<p>A: Among the model in (i) and that in (k), the one in (k) has lower
AIC and the residual deviances in both are of reasonably. Hence we
prefer the model in (k) here.</p>
</div>
<div id="n-2" class="section level2" number="4.14">
<h2><span class="header-section-number">4.14</span> n)</h2>
<p>Q: We will now evaluate the probability of Biden winning the 2020
election with this model where Biden is the Democratic nominee and Trump
is the Republican nominee. What should be the corresponding INC, RUN and
DUR variables?</p>
<p>A: The variables <code>INC, RUN, DUR</code> are <code>-1,-1,0</code>.
Biden is the Democratic nominee (INC = -1), incumbent president is
Republican (RUN = -1) and has been in power for 1 term (DUR = 0).</p>
</div>
<div id="o-2" class="section level2" number="4.15">
<h2><span class="header-section-number">4.15</span> o)</h2>
<p>Q: The projected growth rate for the US economy for 2020 is -5%
(possibly worse). Based on this, what is the probability of Joe Biden
winning in the 2020 election based on the model you developed in
question (k)?</p>
<p>A: Using our model we need variable for growth. US per capita GDP is
supposed to shrink by at least 5% in 2020. Since the incumbent is
republican we have <code>GROWTH = (-5)*(-1)=5</code>.</p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="#cb189-1" aria-hidden="true" tabindex="-1"></a>newdat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">RUN=</span><span class="sc">-</span><span class="dv">1</span>, <span class="at">GROWTH=</span><span class="dv">5</span>, <span class="at">DUR=</span><span class="dv">0</span>)</span>
<span id="cb189-2"><a href="#cb189-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(model3, newdat, <span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<pre><code>##         1 
## 0.6226581</code></pre>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="#cb191-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Or manually calculate</span></span>
<span id="cb191-2"><a href="#cb191-2" aria-hidden="true" tabindex="-1"></a><span class="co"># data20 &lt;- c(-1,5,-0)</span></span>
<span id="cb191-3"><a href="#cb191-3" aria-hidden="true" tabindex="-1"></a><span class="co"># exp(sum(model3$coefficients * data20)) / (1 + exp(sum(model3$coefficients * data20)))</span></span></code></pre></div>
<p>The model predicts that the Democratic nominee Joe Biden wins with
~62% probability.</p>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>See <a href="https://stackoverflow.com/a/10913296" class="uri">https://stackoverflow.com/a/10913296</a><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
</section>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
