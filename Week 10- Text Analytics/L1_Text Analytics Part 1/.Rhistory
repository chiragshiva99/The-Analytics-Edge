twitter <- read.csv("twitter.csv",stringsAsFactors=FALSE)
head(twitter)
str(twitter)
summary(twitter)
View(twitter)
View(twitter)
# Data Prep
# 1. checking negative tweets
twitter$Neg <- as.factor(twitter$sentiment <= 2)
table(twitter$Neg)
# Load Libraries
library(tm)
# Load Libraries
library(tm)
library(NLP)
library(NLP)
?Corpus
# 2. Data pre-processing using tm library
?Corpus
# 2. Data pre-processing using tm library
?Corpus
?Corpus
# Load Libraries
library(tm)
library(NLP)
?Corpus
corpus <- Corpus(VectorSource(twitter$tweet))
View(corpus)
View(corpus)
corpus[[1]]
as.character(corpus[[1]])
corpus[[1]]
as.character(corpus[[2664]])
?tm_map
getTransformations()
View(corpus)
View(corpus)
# 2.1 converting text to lower case
corpus <- tm_map(corpus,content_transformer(tolower))
corpus <- tm_map(corpus,tolower) # alternative command
corpus <- tm_map(corpus, function(x) iconv(enc2utf8(x), sub = "byte"))
View(corpus)
View(corpus)
corpus <- tm_map(corpus, content_transformer(function(x) iconv(enc2utf8(x), sub = "bytes")))
View(corpus)
View(corpus)
corpus <- tm_map(corpus, content_transformer(function(x)    iconv(enc2utf8(x), sub = "bytes")))
corpus <- tm_map(corpus, content_transformer(tolower))
as.character(corpus[[2664]])
View(corpus)
?stopwords
# 2.2 remove stopwords
stopwords('en')
# 2.2 remove stopwords
stopwords('english')
# 2.2 remove stopwords
stopwords(kind = "en")
corpus <- tm_map(corpus, removeWords, stopwords(kind = "en"))
# 2.2 remove stopwords
stopwords(kind = "en")
corpus <- tm_map(corpus, removeWords, stopwords(kind = "en"))
as.character(corpus[[1]])
# 2.1 converting text to lower case
corpus <- tm_map(corpus, function(x) iconv(enc2utf8(x), sub = "byte"))
corpus <- tm_map(corpus, content_transformer(function(x)    iconv(enc2utf8(x), sub = "bytes")))
corpus <- tm_map(corpus, content_transformer(tolower))
as.character(corpus[[1]])
# 2.2 remove stopwords
stopwords(kind = "en")
corpus <- tm_map(corpus, removeWords, stopwords(kind = "en"))
as.character(corpus[[1]])
#manually removing words
corpus <- tm_map(corpus,removeWords,c("drive","driving","driver","self-driving","car","cars"))
as.character(corpus[[1]])
library(SnowballC)
# 2.4 Stemming using SnowballC package
corpus <- tm_map(corpus,stemDocument)
as.character(corpus[[1]])
twitter$tweet[1]
# 2.5 Create DTM
?DocumentTermMatrix
# 2.5 Create DTM
?DocumentTermMatrix
dtm <- DocumentTermMatrix(corpus)
View(dtm)
dtm[1,]
inspect(dtm[1,])
# 2.6 Removing sparse terms
?findFreqTerms
dtm[,"accid"]
# 2.6 Removing sparse terms
?findFreqTerms
dtm[,"accid"]
findFreqTerms(dtm,lowfreq=50) #words with a higher frequency
findFreqTerms(dtm,lowfreq=10) #words with a higher frequency
findFreqTerms(dtm,lowfreq=50) #words with a higher frequency
dtm[,"accid"]
dtm[,"awesom"]
dtm[,"accid"]
dtm[,"awesom"]
?removeSparseTerms
dtm <- removeSparseTerms(dtm,0.995)
dtm
View(dtm)
View(dtm)
# 3. Prep model for learning
twittersparse <- as.data.frame(as.metrix(dtm))
# 3. Prep model for learning
twittersparse <- as.data.frame(as.matrix(dtm))
View(twittersparse)
View(dtm)
# str(twittersparse)
colnames(twittersparse)
str(twittersparse)
View(corpus)
View(twittersparse)
View(twittersparse)
# str(twittersparse)
# colnames(twittersparse)
colnames(twittersparse) <- make.names(colnames(twittersparse))
colnames(twittersparse)
# Get word counts in decreasing order
word_freqs = sort(colSums(twittersparse), decreasing=TRUE)
# Create data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=unname(word_freqs))
View(dm)
View(dm)
# Plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
library(wordcloud)
# Plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
# Plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
?wordcloud
# Plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
View(twittersparse)
View(twittersparse)
View(twitter)
View(twitter)
twitter$Neg
twittersparse$Neg <- twitter$Neg
twittersparse$Neg <- twitter$Neg
str(twittersparse)
twittersparse$Neg <- twitter$Neg
str(twittersparse)
View(twittersparse)
spl <- sample.split(twittersparse$Neg,SplitRatio=0.7)
spl <- sample.split(twittersparse$Neg,SplitRatio=0.7)
train <- subset(twittersparse,spl==TRUE)
test <- subset(twittersparse,spl==FALSE)
library(caTools)
# 4 Train and test a classifier
set.seed(123)
spl <- sample.split(twittersparse$Neg,SplitRatio=0.7)
train <- subset(twittersparse,spl==TRUE)
test <- subset(twittersparse,spl==FALSE)
View(train)
View(train)
# 4.1 Logistc regression
model1 <- glm(Neg~.,data=train,family=binomial)
summary(model1)
# 4.1 Logistc regression
model1 <- glm(Neg~.,data=train,family=binomial)
# 4 Train and test a classifier
set.seed(123)
spl <- sample.split(twittersparse$Neg,SplitRatio=0.7)
train <- subset(twittersparse,spl==TRUE)
test <- subset(twittersparse,spl==FALSE)
# 4.1 Logistc regression
model1 <- glm(Neg~.,data=train,family=binomial)
rm(list=ls())
# Load Libraries
library(tm)
library(NLP)
library(SnowballC)
library(wordcloud)
library(caTools)
# Read csv
twitter <- read.csv("twitter.csv",stringsAsFactors=FALSE)
head(twitter)
str(twitter)
summary(twitter)
# Data Prep
#checking negative tweets
twitter$Neg <- as.factor(twitter$sentiment <= 2)
table(twitter$Neg)
# Data pre-processing using tm library
?Corpus
?VectorSource
corpus <- Corpus(VectorSource(twitter$tweet))
corpus[[1]]
as.character(corpus[[1]])
as.character(corpus[[2664]])
getTransformations()
# 2.1 converting text to lower case
corpus <- tm_map(corpus, function(x) iconv(enc2utf8(x), sub = "byte"))
corpus <- tm_map(corpus, content_transformer(function(x)    iconv(enc2utf8(x), sub = "bytes")))
corpus <- tm_map(corpus, content_transformer(tolower))
as.character(corpus[[1]])
# 2.2 remove stopwords
stopwords(kind = "en")
corpus <- tm_map(corpus, removeWords, stopwords(kind = "en"))
as.character(corpus[[1]])
#manually removing words
corpus <- tm_map(corpus,removeWords,c("drive","driving","driver","self-driving","car","cars"))
as.character(corpus[[1]])
# 2.3 remove punctuations
corpus <- tm_map(corpus,removePunctuation)
as.character(corpus[[1]])
# 2.4 Stemming using SnowballC package
corpus <- tm_map(corpus,stemDocument)
as.character(corpus[[1]])
twitter$tweet[1] #check to compare how much has the tweet changed
# 2.5 Create DTM
?DocumentTermMatrix
dtm <- DocumentTermMatrix(corpus)
dtm #cols with all characters
dtm[1,]
inspect(dtm[1,])
# 2.6 Removing sparse terms
?findFreqTerms
dtm[,"accid"]
findFreqTerms(dtm,lowfreq=50) #words with a higher frequency
dtm[,"accid"]
dtm[,"awesom"]
?removeSparseTerms
dtm <- removeSparseTerms(dtm,0.995)
dtm
# 3. Prep model for learning
twittersparse <- as.data.frame(as.matrix(dtm))
# str(twittersparse)
# colnames(twittersparse)
colnames(twittersparse) <- make.names(colnames(twittersparse))
colnames(twittersparse)
# Get word counts in decreasing order
word_freqs = sort(colSums(twittersparse), decreasing=TRUE)
# Create data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=unname(word_freqs))
# Plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
twittersparse$Neg <- twitter$Neg
str(twittersparse)
# 4 Train and test a classifier
set.seed(123)
spl <- sample.split(twittersparse$Neg,SplitRatio=0.7)
train <- subset(twittersparse,spl==TRUE)
test <- subset(twittersparse,spl==FALSE)
# 4.1 Logistc regression
model1 <- glm(Neg~.,data=train,family=binomial)
summary(model1)
predict1 <- predict(model1,newdata=test,type="response")
table(predict1>=0.5,test$Neg)
library(rpart)
library(rpart.plot)
# 4.2 CART
model2 <- rpart(Neg~.,data=train)
summary(model2)
prp(model2,type=4,extra=4) # Here, TRUE indicates Neg = 1, FALSE indicates Neg = 0.
predict2 <- predict(model2,newdata=test,type="class")
table(predict2,test$Neg)
model2a <- rpart(Neg~.,data=train,cp=10^-6)
prp(model2a,type=4,extra=4)
model2b <- prune(model2a,cp=0.00092251) # To prune, we use the smallest value of xerror
prp(model2b,type=4,extra=4)
predict2b <- predict(model2b,newdata=test,type="class") #making predictions using the test dataset
table(predict2b,test$Neg)
table(predict2b,test$Neg)
library(randomForest)
# We use the default parameters to fit the model (500 trees)
model3 <- randomForest(Neg~.,data=train)
summary(model3)
# We use the default parameters to fit the model (500 trees)
model3 <- randomForest(Neg~.,data=train)
summary(model3)
model3
predict3 <- predict(model3,newdata=test,type="class")
table(predict3,test$Neg)
# Load the wordcloud package
if(!require(wordcloud)){
install.packages("wordcloud")
library(wordcloud)
}
# Get word counts in decreasing order
word_freqs = sort(colSums(twittersparse), decreasing=TRUE)
rm(list=ls()) # Clear the environment
# setwd("...")  # Setup the working directory
twitter <- read.csv("twitter.csv")
str(twitter) # Internal structure of the dataframe
twitter <- read.csv("twitter.csv",stringsAsFactors=FALSE)
# twitter <- read.csv("twitter.csv",stringsAsFactors=FALSE,fileEncoding = "latin1") Alternative command for forcing the file encoding
str(twitter)  # 2664 observations of 2 variables
# head(twitter) # First part of the dataframe
# tail(twitter) # Last part of the dataframe
summary(twitter) # Summary of the data
twitter$Neg <- as.factor(twitter$sentiment<=2)
table(twitter$Neg)
if(!require(tm)){
install.packages("tm")
library(tm)
}
?Corpus
?VectorSource
corpus <- Corpus(VectorSource(twitter$tweet))
# corpus
corpus[[1]]
as.character(corpus[[1]])
corpus[[2664]]
as.character(corpus[[2664]])
?tm_map
getTransformations()
# corpus <- tm_map(corpus,content_transformer(tolower))
# corpus <- tm_map(corpus,tolower) # alternative command
corpus <- tm_map(corpus, function(x) iconv(enc2utf8(x), sub = "byte"))
corpus <- tm_map(corpus, content_transformer(function(x)    iconv(enc2utf8(x), sub = "bytes")))
corpus <- tm_map(corpus, content_transformer(tolower))
as.character(corpus[[2664]])
as.character(corpus[[1]])
stopwords("english")
corpus <- tm_map(corpus,removeWords,stopwords("english"))
# And ... Let's check a couple of documents
as.character(corpus[[1]])
as.character(corpus[[2664]])
corpus <- tm_map(corpus,removeWords,c("drive","driving","driver","self-driving","car","cars"))
# And ... Let's check a couple of documents
as.character(corpus[[1]])
as.character(corpus[[2664]])
corpus <- tm_map(corpus,removePunctuation)
# And ... Let's check a couple of documents
as.character(corpus[[1]])
as.character(corpus[[2664]])
if(!require(SnowballC)){
install.packages("SnowballC")
library(SnowballC)
}
corpus <- tm_map(corpus,stemDocument)
# And ... Let's check a couple of documents
as.character(corpus[[1]])
as.character(corpus[[2664]])
twitter$tweet[1]
as.character(corpus[[1]])
?DocumentTermMatrix
dtm <- DocumentTermMatrix(corpus)
dtm
dtm[1,]
inspect(dtm[1,])
inspect(dtm[2664,])
?findFreqTerms
# With this function, we find term appearing with frequency lower than 50
dtm[,"accid"]
findFreqTerms(dtm,lowfreq=50)
dtm[,"accid"]
dtm[,"awesom"]
?removeSparseTerms
dtm <- removeSparseTerms(dtm,0.995)
dtm #
twittersparse <- as.data.frame(as.matrix(dtm))
# str(twittersparse)
# colnames(twittersparse)
colnames(twittersparse) <- make.names(colnames(twittersparse))
# colnames(twittersparse)
# Load the wordcloud package
if(!require(wordcloud)){
install.packages("wordcloud")
library(wordcloud)
}
# Get word counts in decreasing order
word_freqs = sort(colSums(twittersparse), decreasing=TRUE)
# Create data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=unname(word_freqs))
# Plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
twittersparse$Neg <- twitter$Neg
str(twittersparse)
# Load the caTools package and set the seed
if(!require(caTools)){
install.packages("caTools")
library(caTools)
}
set.seed(123)
# Create train and test sets (with balanced response)
spl <- sample.split(twittersparse$Neg,SplitRatio=0.7)
train <- subset(twittersparse,spl==TRUE)
test <- subset(twittersparse,spl==FALSE)
model1 <- glm(Neg~.,data=train,family=binomial)
summary(model1)
predict1 <- predict(model1,newdata=test,type="response")
table(predict1>=0.5,test$Neg)
library(rpart)
library(rpart.plot)
model2 <- rpart(Neg~.,data=train)
# summary(model2)
# model2
prp(model2,type=4,extra=4) # Here, TRUE indicates Neg = 1, FALSE indicates Neg = 0.
predict2 <- predict(model2,newdata=test,type="class")
table(predict2,test$Neg)
model2a <- rpart(Neg~.,data=train,cp=10^-6)
prp(model2a,type=4,extra=4)
printcp(model2a)
# plotcp(model2a)
model2b <- prune(model2a,cp=0.00092251) # To prune, we use the smallest value of xerror
prp(model2b,type=4,extra=4)
predict2b <- predict(model2b,newdata=test,type="class")
table(predict2b,test$Neg)
library(randomForest)
# We use the default parameters to fit the model (500 trees)
model3 <- randomForest(Neg~.,data=train)
# summary(model3)
model3
predict3 <- predict(model3,newdata=test,type="class")
table(predict3,test$Neg)
rm(list=ls()) # Clear the environment
# setwd("...")  # Setup the working directory
twitter <- read.csv("twitter.csv")
str(twitter) # Internal structure of the dataframe
twitter <- read.csv("twitter.csv",stringsAsFactors=FALSE)
# twitter <- read.csv("twitter.csv",stringsAsFactors=FALSE,fileEncoding = "latin1") Alternative command for forcing the file encoding
str(twitter)  # 2664 observations of 2 variables
# head(twitter) # First part of the dataframe
# tail(twitter) # Last part of the dataframe
summary(twitter) # Summary of the data
twitter$Neg <- as.factor(twitter$sentiment<=2)
table(twitter$Neg)
if(!require(tm)){
install.packages("tm")
library(tm)
}
?Corpus
?VectorSource
corpus <- Corpus(VectorSource(twitter$tweet))
# corpus
corpus[[1]]
as.character(corpus[[1]])
corpus[[2664]]
as.character(corpus[[2664]])
?tm_map
getTransformations()
# corpus <- tm_map(corpus,content_transformer(tolower))
# corpus <- tm_map(corpus,tolower) # alternative command
corpus <- tm_map(corpus, function(x) iconv(enc2utf8(x), sub = "byte"))
corpus <- tm_map(corpus, content_transformer(function(x)    iconv(enc2utf8(x), sub = "bytes")))
corpus <- tm_map(corpus, content_transformer(tolower))
as.character(corpus[[2664]])
as.character(corpus[[1]])
stopwords("english")
corpus <- tm_map(corpus,removeWords,stopwords("english"))
# And ... Let's check a couple of documents
as.character(corpus[[1]])
as.character(corpus[[2664]])
corpus <- tm_map(corpus,removeWords,c("drive","driving","driver","self-driving","car","cars"))
# And ... Let's check a couple of documents
as.character(corpus[[1]])
as.character(corpus[[2664]])
corpus <- tm_map(corpus,removePunctuation)
# And ... Let's check a couple of documents
as.character(corpus[[1]])
as.character(corpus[[2664]])
if(!require(SnowballC)){
install.packages("SnowballC")
library(SnowballC)
}
corpus <- tm_map(corpus,stemDocument)
# And ... Let's check a couple of documents
as.character(corpus[[1]])
as.character(corpus[[2664]])
twitter$tweet[1]
as.character(corpus[[1]])
?DocumentTermMatrix
dtm <- DocumentTermMatrix(corpus)
dtm
dtm[1,]
inspect(dtm[1,])
inspect(dtm[2664,])
?findFreqTerms
# With this function, we find term appearing with frequency lower than 50
dtm[,"accid"]
findFreqTerms(dtm,lowfreq=50)
dtm[,"accid"]
dtm[,"awesom"]
?removeSparseTerms
dtm <- removeSparseTerms(dtm,0.995)
dtm #
twittersparse <- as.data.frame(as.matrix(dtm))
# str(twittersparse)
# colnames(twittersparse)
colnames(twittersparse) <- make.names(colnames(twittersparse))
# colnames(twittersparse)
# Load the wordcloud package
if(!require(wordcloud)){
install.packages("wordcloud")
library(wordcloud)
}
# Get word counts in decreasing order
word_freqs = sort(colSums(twittersparse), decreasing=TRUE)
# Create data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=unname(word_freqs))
# Plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
twittersparse$Neg <- twitter$Neg
str(twittersparse)
# Load the caTools package and set the seed
if(!require(caTools)){
install.packages("caTools")
library(caTools)
}
set.seed(123)
# Create train and test sets (with balanced response)
spl <- sample.split(twittersparse$Neg,SplitRatio=0.7)
train <- subset(twittersparse,spl==TRUE)
test <- subset(twittersparse,spl==FALSE)
View(train)
