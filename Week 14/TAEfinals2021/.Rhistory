# Load all packages
library(caTools)      # For properly splitting data into training and test sets
library(rpart)        # For CARTs
library(rpart.plot)   # For visualizing CARTs
library(randomForest) # For Random Forests
library(caret)        # For multiple ML functions
library(flexclust)    # For processing the results of cluster analysis
library(e1071)        # Naive Bayes Classifier
# Remove all variables from the R environment to create a fresh start
rm(list=ls())
# Create dataframe
exm <- read.csv("extramarital.csv")
View(exm)
View(exm)
table(as.integer(exm$time_in_affairs > 0))
?tapply
table(exm$time_in_affairs, exm$marriage_rating)
a <- table(exm$time_in_affairs, exm$marriage_rating)
a <- df(table(exm$time_in_affairs, exm$marriage_rating))
?df
df(table(exm$time_in_affairs, exm$marriage_rating))
df(table(exm$time_in_affairs, exm$marriage_rating))
df(table(exm$time_in_affairs, exm$marriage_rating))
table(exm$time_in_affairs, exm$marriage_rating)
table(exm$time_in_affairs, exm$marriage_rating)
table(exm$time_in_affairs > 0, exm$marriage_rating)
a <- table(exm$time_in_affairs > 0, exm$marriage_rating)
sum(a[1,1])
a[1,1] / (a[1,1] + a[2,1])
a[2,1] / (a[1,1] + a[2,1])
a[2,1] / (a[1,1] + a[2,1])
a[2,2] / (a[1,2] + a[2,2])
a[2,3] / (a[1,3] + a[2,3])
a[2,4] / (a[1,4] + a[2,4])
a[2,5] / (a[1,5] + a[2,5])
table(exm$time_in_affairs, exm$yrs_married)
table(exm$time_in_affairs>0, exm$yrs_married)
table(exm$time_in_affairs, exm$marriage_rating)
a <- table(exm$time_in_affairs > 0, exm$marriage_rating)
a[2,1] / (a[1,1] + a[2,1])
table(exm$time_in_affairs > 0, exm$marriage_rating)
table(exm$time_in_affairs>0, exm$yrs_married)
table(exm$time_in_affairs, exm$yrs_married)
table(exm$time_in_affairs>0, exm$yrs_married)
b <- table(exm$time_in_affairs>0, exm$yrs_married)
table(exm$time_in_affairs > 0, exm$marriage_rating)
table(exm$time_in_affairs>0, exm$yrs_married)
b <- table(exm$time_in_affairs>0, exm$yrs_married)
b[2,1] / (b[1,1] + b[2,1])
b[2,2] / (b[1,2] + b[2,2])
b[2,3] / (b[1,3] + b[2,3])
b[2,4] / (b[1,4] + b[2,4])
b[2,5] / (b[1,5] + b[2,5])
tapply(as.integer(exm$time_in_affairs > 0), exm$yrs_married, mean)
# ?cor
pairs.panels(exm,ellipses = F, lm =T, breaks=10, hist.col="blue")
# ?cor
library(psych)
pairs.panels(exm,ellipses = F, lm =T, breaks=10, hist.col="blue")
cor(exm[,1:7])
cor(exm[,1:7]) - diag(rep(1, 7)) # remove diagonals
max(cor(exm[,1:7]) - diag(rep(1, 7))) # 0.8940818
cor(exm)
View(exm)
plot(exm$time_in_affairs)
exm$affair <- factor(ifelse(exm$time_in_affairs == 0, "No", ifelse(exm$time_in_affairs >= 5, "Major", "Minor")))
table(exm$affair)
set.seed(100)
set.seed(100)
spl   <- sample.split(exm$affair,SplitRatio=0.7)
train <- subset(exm,spl==TRUE);
test  <- subset(exm,spl==FALSE);
set.seed(100)
cart1 <- rpart(affair ~ marriage_rating +
age +
yrs_married +
religiosity +
education +
occupation ,data=train, method = "class")
prp(cart1) # double check answer
prp(cart1,type=1)
prp(cart1,type=4)
prp(cart1,type=4,extra=4)
fancyRpartPlot(cart1)
library(rattle)
library(RColorBrewer)
fancyRpartPlot(cart1)
predict_cart1 <- predict(cart1,newdata=test, type = "class")
pred_table_cart1 <- table(test$affair, predict_cart1)
pred_table_cart1
printcp(cart1)
set.seed(100)
cart2 <- rpart(affair ~ marriage_rating +
age +
yrs_married +
religiosity +
education +
occupation ,data=train, method="class", cp = 0.0001)
cart2$frame["1", 1]
printcp(cart2)
fancyRpartPlot(cart2)
cart2$frame["1", 1]
printcp(cart2)
set.seed(100)
cart3 <- rpart(affair ~ marriage_rating +
age +
yrs_married +
religiosity +
education +
occupation ,data=train, method="class", cp = 0.00278358)
prp(cart3) # 3 splits
fancyRpartPlot(cart3)
predict_cart3 <- predict(cart3,newdata=test, type = "class")
pred_table_cart3 <- table(test$affair, predict_cart3)
pred_table_cart3
Accuracy <- (185+1188)/1910
Accuracy
set.seed(100)
forest <- randomForest(affair ~ marriage_rating +
age +
yrs_married +
religiosity +
education +
occupation ,data=train, method = "class")
forest
predict_forest <- predict(forest,newdata=test, type = "class")
pred_table_forest <- table(test$affair, predict_forest)
pred_table_forest
Accuracy <- (241+1134)/1910
Accuracy # 0.7198953
varImpPlot(forest)
importance(forest)
predict_forest <- predict(forest,newdata=test, type = "class")
pred_table_forest <- table(test$affair, predict_forest)
pred_table_forest
Accuracy <- (241+1134)/1910
Accuracy # 0.7198953
varImpPlot(forest)
importance(forest)
set.seed(100)
cart2 <- rpart(affair ~ marriage_rating +
age +
yrs_married +
religiosity +
education +
occupation ,data=train, method="class", cp = 0.0001)
cart2$frame["1", 1]
printcp(cart2)
set.seed(100)
cart3 <- rpart(affair ~ marriage_rating +
age +
yrs_married +
religiosity +
education +
occupation ,data=train, method="class", cp = 0.00278358)
prp(cart3) # 3 splits
fancyRpartPlot(cart3)
predict_cart3 <- predict(cart3,newdata=test, type = "class")
pred_table_cart3 <- table(test$affair, predict_cart3)
pred_table_cart3
Accuracy <- (185+1188)/1910
Accuracy
set.seed(100)
forest <- randomForest(affair ~ marriage_rating +
age +
yrs_married +
religiosity +
education +
occupation ,data=train, method = "class")
forest
predict_forest <- predict(forest,newdata=test, type = "class")
pred_table_forest <- table(test$affair, predict_forest)
pred_table_forest
Accuracy <- (241+1134)/1910
Accuracy # 0.7198953
varImpPlot(forest)
importance(forest)
# Remove all variables from the R environment to create a fresh start
rm(list=ls())
# Create dataframe
weatherAUS <- read.csv("weatherAUS.csv")
# Load all packages
library(caTools)      # For properly splitting data into training and test sets
library(rpart)        # For CARTs
library(rpart.plot)   # For visualizing CARTs
library(randomForest) # For Random Forests
library(caret)        # For multiple ML functions
library(flexclust)    # For processing the results of cluster analysis
library(e1071)        # Naive Bayes Classifier
# Remove all variables from the R environment to create a fresh start
rm(list=ls())
# Create dataframe
exm <- read.csv("extramarital.csv")
table(as.integer(exm$time_in_affairs > 0))
2053/(2053+4313) #0.3224945
table(exm$time_in_affairs, exm$marriage_rating)
table(exm$time_in_affairs > 0, exm$marriage_rating)
a <- table(exm$time_in_affairs > 0, exm$marriage_rating)
a[2,1] / (a[1,1] + a[2,1])
a[2,2] / (a[1,2] + a[2,2])
a[2,3] / (a[1,3] + a[2,3])
a[2,4] / (a[1,4] + a[2,4])
a[2,5] / (a[1,5] + a[2,5])
tapply(as.integer(exm$time_in_affairs > 0), exm$marriage_rating, mean)
table(exm$time_in_affairs, exm$yrs_married)
table(exm$time_in_affairs>0, exm$yrs_married)
b <- table(exm$time_in_affairs>0, exm$yrs_married)
b[2,1] / (b[1,1] + b[2,1])
b[2,2] / (b[1,2] + b[2,2])
b[2,3] / (b[1,3] + b[2,3])
b[2,4] / (b[1,4] + b[2,4])
b[2,5] / (b[1,5] + b[2,5])
b[2,6] / (b[1,6] + b[2,6])
b[2,7] / (b[1,7] + b[2,7])
tapply(as.integer(exm$time_in_affairs > 0), exm$yrs_married, mean)
# ?cor
library(psych)
pairs.panels(exm,ellipses = F, lm =T, breaks=10, hist.col="blue")
cor(exm)
cor(exm[,1:7])
cor(exm[,1:7]) - diag(rep(1, 7)) # remove diagonals
max(cor(exm[,1:7]) - diag(rep(1, 7))) # 0.8940818
plot(exm$time_in_affairs)
exm$affair <- factor(ifelse(exm$time_in_affairs == 0, "No", ifelse(exm$time_in_affairs >= 5, "Major", "Minor")))
table(exm$affair)
149/(4313+1904+149) #0.02340559
set.seed(100)
spl   <- sample.split(exm$affair,SplitRatio=0.7)
train <- subset(exm,spl==TRUE);
test  <- subset(exm,spl==FALSE);
set.seed(100)
cart1 <- rpart(affair ~ marriage_rating +
age +
yrs_married +
religiosity +
education +
occupation ,data=train, method = "class")
cart1$frame["1", 1]
prp(cart1) # double check answer
prp(cart1,type=1)
prp(cart1,type=4)
prp(cart1,type=4,extra=4)
library(rattle)
library(RColorBrewer)
fancyRpartPlot(cart1)
predict_cart1 <- predict(cart1,newdata=test, type = "class")
pred_table_cart1 <- table(test$affair, predict_cart1)
pred_table_cart1
4+41+202+369+119+1175 # number of data in test set # 1910
Accuracy <- (202+1175)/1910
Accuracy
printcp(cart1)
set.seed(100)
cart2 <- rpart(affair ~ marriage_rating +
age +
yrs_married +
religiosity +
education +
occupation ,data=train, method="class", cp = 0.0001)
cart2$frame["1", 1]
printcp(cart2)
set.seed(100)
cart3 <- rpart(affair ~ marriage_rating +
age +
yrs_married +
religiosity +
education +
occupation ,data=train, method="class", cp = 0.00278358)
prp(cart3) # 3 splits
fancyRpartPlot(cart3)
predict_cart3 <- predict(cart3,newdata=test, type = "class")
pred_table_cart3 <- table(test$affair, predict_cart3)
pred_table_cart3
Accuracy <- (185+1188)/1910
Accuracy
set.seed(100)
forest <- randomForest(affair ~ marriage_rating +
age +
yrs_married +
religiosity +
education +
occupation ,data=train, method = "class")
forest
predict_forest <- predict(forest,newdata=test, type = "class")
pred_table_forest <- table(test$affair, predict_forest)
pred_table_forest
Accuracy <- (241+1134)/1910
Accuracy # 0.7198953
varImpPlot(forest)
importance(forest)
# Remove all variables from the R environment to create a fresh start
rm(list=ls())
# Create dataframe
weatherAUS <- read.csv("weatherAUS.csv")
View(weatherAUS)
# Remove missing entries
weatherAUS <- na.omit(weatherAUS)
# Create train and test datasets
idx <- c(1:2977)
spl <- c(1:2977)
spl[idx <= 2000] <- "TRUE"
spl[idx  > 2000] <- "FALSE"
train <- subset(weatherAUS,spl==TRUE)
test  <- subset(weatherAUS,spl==FALSE)
# Create limitedTrain and limitedTest
limitedTrain <- train
limitedTrain$RainTomorrow  <- NULL
limitedTrain$Date  <- NULL
limitedTest <- test
limitedTest$RainTomorrow <- NULL
limitedTest$Date <- NULL
View(limitedTrain)
View(train)
limitedTest$Date <- NULL
# Normalize
preproc   <- preProcess(limitedTrain)
normTrain <- predict(preproc, limitedTrain)
normTest  <- predict(preproc, limitedTest)
View(preproc)
View(normTest)
View(normTrain)
View(train)
distances <- dist(normTrain, method="euclidean")
clusterWeather1 <- hclust(distances, method="ward.D2")
plot(clusterWeather1)
clusterWeather1d  <- cutree(clusterWeather1, h = 10)
table(clusterWeather1d) # 38 clusters in total
plot(clusterWeather1d)
clusterWeather1 <- hclust(distances, method="ward.D2")
plot(clusterWeather1)
clusterWeather1d  <- cutree(clusterWeather1, h = 10)
plot(clusterWeather1d)
table(clusterWeather1d) # 38 clusters in total
# plot(clusterWeather1d)
table(clusterWeather1d) # 38 clusters in total
clusterWeather2 <- hclust(distances, method="complete")
plot(clusterWeather2)
clusterWeather2d  <- cutree(clusterWeather2, h = 10)
table(clusterWeather2d) # 5 clusters in total
set.seed(100)
km <- kmeans(normTrain, centers=3, nstart=50)
table(km$cluster)
km.kcca      <- as.kcca(km, normTrain)
clusterTrain <- predict(km.kcca)
clusterTest  <- predict(km.kcca, newdata=normTest)
table(clusterTest)
train1 <- subset(train, clusterTrain == 1)
train2 <- subset(train, clusterTrain == 2)
train3 <- subset(train, clusterTrain == 3)
test1 <- subset(test, clusterTest == 1)
test2 <- subset(test, clusterTest == 2)
test3 <- subset(test, clusterTest == 3)
table(train1$RainTomorrow)
table(train2$RainTomorrow)
table(train3$RainTomorrow)
model1 <- naiveBayes(as.factor(RainTomorrow) ~ MinTemp + MaxTemp + Rainfall +
WindSpeed9am + WindSpeed3pm + Humidity9am + Humidity3pm +
Pressure9am + Pressure3pm + Temp9am + Temp3pm,
data=train1)
PredictTest1 <- predict(model1, newdata = test1, type="class")
pred_table <- table(PredictTest1,test1$RainTomorrow)
Accuracy <- (pred_table[1,1]+pred_table[2,2])/sum(pred_table)
Accuracy # of model1 is 0.6936416
AllPredictions <- c(PredictTest1, PredictTest2, PredictTest3)
ALlOutcomes <- c(test1$RainTomorrow,
test2$RainTomorrow,
test3$RainTomorrow) # actual results
pred_table <- table(AllPredictions,AllOutcomes)
pred_table
Accuracy <- (pred_table[1,1]+pred_table[2,2])/sum(pred_table)
Accuracy # overall accuracy is 0.8742004
